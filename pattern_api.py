#!/usr/bin/env python3
"""
Pattern Detection API Server
ì›¹ ëŒ€ì‹œë³´ë“œì™€ íŒ¨í„´ ê²€ì¶œ ëª¨ë¸ì„ ì—°ê²°í•˜ëŠ” Flask API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import sys
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (api ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ ëª…ì‹œì  ë¡œë“œ)
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '.env'))
import numpy as np
import pandas as pd
import torch
from datetime import datetime, timedelta, timezone
import ccxt
import logging
import pytz
import pickle
import jwt
import sqlite3
import json
import yagmail
import random
import string
import math

# ê¶Œí•œ ì‹œìŠ¤í…œ import
from database import db_manager, UserTier
from auth_middleware import require_auth, get_tier_limits
from scheduler import AutoAnalysisScheduler
from performance_cache import performance_cache, cached, task_manager

# íŒ¨í„´ ê²€ì¶œ ìŠ¤í¬ë¦½íŠ¸ import
sys.path.append('/home/andy/candle-model/scripts')
from pattern_detecter_with_cache_v8_64emd_no_reranker import (
    load_ohlc_data, normalize_window, PatternEncoder, 
    find_similar_patterns, precompute_and_save_embeddings,
    train_or_load_model, save_cache_atomically, collate_fn
)


from flasgger import Swagger

app = Flask(__name__)
# Swagger Configuration
app.config['SWAGGER'] = {
    'title': 'Candle Pattern Finder API',
    'uiversion': 3,
    'description': 'API for cryptocurrency candle pattern analysis and similarity search',
    'version': '1.0.0',
    'specs_route': '/apidocs/'
}
swagger = Swagger(app)

app.config['JWT_SECRET_KEY'] = os.getenv('JWT_SECRET_KEY', 'temp-dev-key-CHANGE-IN-PRODUCTION')
# CORS(app)
CORS(app, supports_credentials=True, origins=[
    'https://pattern-finder.com',
    'http://localhost:3000',
    'http://127.0.0.1:3000'
]) # í—ˆê°€ëœ ë„ë©”ì¸ë§Œ ì ‘ê·¼ í—ˆìš©

# Global variables for loaded model and data
model = None
# reranker_model = None  # Removed in v8
embedding_data = None
# reranker_cache = None # Removed in v8
full_ohlc_data = None
full_time_data = None
full_ohlc_data = None
full_time_data = None
binance_exchange = None
auto_scheduler = None

# ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ì„ì‹œ ì €ì¥ì†Œ (ì‹¤ì œ ìš´ì˜í™˜ê²½ì—ì„œëŠ” Redis ë“± ì‚¬ìš© ê¶Œì¥)
verification_codes = {}

# ì´ë©”ì¼ ë°œì†¡ ì„¤ì •
def get_email_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì´ë©”ì¼ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'email': os.getenv('EMAIL_USER'),
        'password': os.getenv('EMAIL_PASSWORD'),
        'smtp_host': os.getenv('EMAIL_HOST', 'smtp.gmail.com'),
        'smtp_port': int(os.getenv('EMAIL_PORT', '587'))
    }

def generate_verification_code():
    """6ìë¦¬ ëœë¤ ì¸ì¦ ì½”ë“œ ìƒì„±"""
    return ''.join(random.choices(string.digits, k=6))

def send_verification_email(email, code):
    """ì¸ì¦ ì½”ë“œ ì´ë©”ì¼ ë°œì†¡"""
    try:
        config = get_email_config()
        if not config['email'] or not config['password']:
            print(f"ê°œë°œ í™˜ê²½: {email}ì˜ ì¸ì¦ ì½”ë“œëŠ” {code}ì…ë‹ˆë‹¤.")
            return True  # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        
        yag = yagmail.SMTP(config['email'], config['password'])
        
        subject = "[Pattern Finder] ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ"
        body = f"""
        <h2>Pattern Finder íšŒì›ê°€ì…</h2>
        <p>ì•ˆë…•í•˜ì„¸ìš”,</p>
        <p>íšŒì›ê°€ì…ì„ ì™„ë£Œí•˜ê¸° ìœ„í•´ ì•„ë˜ ì¸ì¦ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:</p>
        <div style="background-color: #f0f8ff; padding: 20px; margin: 20px 0; text-align: center; border-radius: 8px;">
            <h1 style="color: #2563eb; font-size: 32px; margin: 0; letter-spacing: 8px;">{code}</h1>
        </div>
        <p>ì´ ì½”ë“œëŠ” 10ë¶„ê°„ ìœ íš¨í•©ë‹ˆë‹¤.</p>
        <p>ê°ì‚¬í•©ë‹ˆë‹¤,<br>Pattern Finder íŒ€</p>
        """
        
        yag.send(to=email, subject=subject, contents=body)
        return True
    except Exception as e:
        print(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
        return False

# Configuration
CONFIG = {
    'csv_path': '/home/andy/candle-model/input_data/New_bs+bn_BTCUSD_250720_update, 240.csv',
    'model_path': '/home/andy/candle-model/output/embeddings/v8/v8_BTC_4H_encoder_multi_emb64.pth',
    'emb_path': '/home/andy/candle-model/output/embeddings/v8/v8_BTC_4H_embeddings_emb64.pkl',
    'emb_dim': 64, # Updated to 64
    'max_pattern_len': 100,
    'min_pattern_len': 3,
    'candidate_count': 100,
    'binance_api_key': os.getenv('BINANCE_API_KEY', ''),
    'binance_api_secret': os.getenv('BINANCE_API_SECRET', '')
}

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì • (UTC+9)
KST = pytz.timezone('Asia/Seoul')

def get_kst_candle_intervals():
    """KST ê¸°ì¤€ 4ì‹œê°„ ìº”ë“¤ ì‹œì‘ ì‹œê°„ë“¤ (1, 5, 9, 13, 17, 21ì‹œ)"""
    return [1, 5, 9, 13, 17, 21]

def get_current_kst_time():
    """í˜„ì¬ KST ì‹œê°„ ë°˜í™˜"""
    return datetime.now(KST)

def find_last_completed_kst_interval(current_kst_time=None):
    """
    í˜„ì¬ KST ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì— ì™„ë£Œëœ 4ì‹œê°„ ìº”ë“¤ êµ¬ê°„ì˜ ë ì‹œê°„ ì°¾ê¸°
    
    Args:
        current_kst_time: KST ì‹œê°„ (ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©)
    
    Returns:
        datetime: ê°€ì¥ ìµœê·¼ ì™„ë£Œëœ êµ¬ê°„ì˜ ë ì‹œê°„ (KST) - íŒ¨í„´ì˜ ì™„ë£Œ ì‹œì 
    """
    if current_kst_time is None:
        current_kst_time = get_current_kst_time()
    
    current_hour = current_kst_time.hour
    
    # 4ì‹œê°„ ìº”ë“¤ êµ¬ê°„: 21-01ì‹œ, 01-05ì‹œ, 05-09ì‹œ, 09-13ì‹œ, 13-17ì‹œ, 17-21ì‹œ
    # ê° êµ¬ê°„ì€ ë§ˆì§€ë§‰ ì‹œê°„(01, 05, 09, 13, 17, 21)ì— ì™„ë£Œë¨
    
    last_completed_hour = None
    target_date = current_kst_time.date()
    
    if current_hour >= 21:  # 21ì‹œ ì´í›„ â†’ 17-21ì‹œ êµ¬ê°„ì´ 21ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 21
    elif current_hour >= 17:  # 17ì‹œ ì´í›„ â†’ 13-17ì‹œ êµ¬ê°„ì´ 17ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 17
    elif current_hour >= 13:  # 13ì‹œ ì´í›„ â†’ 09-13ì‹œ êµ¬ê°„ì´ 13ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 13
    elif current_hour >= 9:   # 9ì‹œ ì´í›„ â†’ 05-09ì‹œ êµ¬ê°„ì´ 09ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 9
    elif current_hour >= 5:   # 5ì‹œ ì´í›„ â†’ 01-05ì‹œ êµ¬ê°„ì´ 05ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 5
    elif current_hour >= 1:   # 1ì‹œ ì´í›„ â†’ ì „ë‚  21-01ì‹œ êµ¬ê°„ì´ 01ì‹œì— ì™„ë£Œë¨
        last_completed_hour = 1
    
    # ì™„ë£Œëœ êµ¬ê°„ì˜ ë ì‹œê°„ ìƒì„±
    result = current_kst_time.replace(
        year=target_date.year,
        month=target_date.month,
        day=target_date.day,
        hour=last_completed_hour,
        minute=0,
        second=0,
        microsecond=0
    )
    
    
    print(f"ğŸ• Current KST: {current_kst_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Last completed interval end: {result.strftime('%Y-%m-%d %H:%M:%S KST')}")
    
    return result

def find_last_completed_candle_time(current_kst_time=None):
    """
    4ì‹œê°„ ìº”ë“¤ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì— ì™„ë£Œëœ ìº”ë“¤ì˜ ì‹œê°„ì„ ë°˜í™˜ (íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤)
    Returns:
        datetime: ê°€ì¥ ìµœê·¼ ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ (KST)
    """
    if current_kst_time is None:
        current_kst_time = get_current_kst_time()
    
    current_hour = current_kst_time.hour
    
    # 4ì‹œê°„ ìº”ë“¤ ì™„ë£Œ ì‹œê°„: 01ì‹œ, 05ì‹œ, 09ì‹œ, 13ì‹œ, 17ì‹œ, 21ì‹œ
    last_completed_candle_hour = None
    target_date = current_kst_time.date()
    
    if current_hour >= 21:  # 21ì‹œ ì´í›„ â†’ 21ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 21
    elif current_hour >= 17:  # 17ì‹œ ì´í›„ â†’ 17ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 17
    elif current_hour >= 13:  # 13ì‹œ ì´í›„ â†’ 13ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 13
    elif current_hour >= 9:   # 9ì‹œ ì´í›„ â†’ 09ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 9
    elif current_hour >= 5:   # 5ì‹œ ì´í›„ â†’ 05ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 5
    elif current_hour >= 1:   # 1ì‹œ ì´í›„ â†’ 01ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 1
    else:  # 1ì‹œ ì´ì „ (0ì‹œëŒ€) â†’ ì „ë‚  21ì‹œ ìº”ë“¤ ì™„ë£Œë¨
        last_completed_candle_hour = 21
        target_date = (current_kst_time - timedelta(days=1)).date()
    
    # ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ ìƒì„±
    result = current_kst_time.replace(
        year=target_date.year,
        month=target_date.month, 
        day=target_date.day,
        hour=last_completed_candle_hour, 
        minute=0, 
        second=0, 
        microsecond=0
    )
    
    print(f"ğŸ• Current KST: {current_kst_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Last completed candle: {result.strftime('%Y-%m-%d %H:%M:%S KST')}")
    
    return result

def find_last_completed_candle_start_time(current_kst_time=None):
    """ê°€ì¥ ìµœê·¼ì— ì™„ë£Œëœ 4ì‹œê°„ ìº”ë“¤ì˜ 'ì‹œì‘' ì‹œê°(KST)ì„ ë°˜í™˜"""
    if current_kst_time is None:
        current_kst_time = get_current_kst_time()
    # ê¸°ì¡´ ìœ í‹¸: ë§ˆì§€ë§‰ ì™„ë£Œ 'ì™„ë£Œ ì‹œê°'(01/05/09/13/17/21)ì„ ë°˜í™˜
    last_completed_end = find_last_completed_kst_interval(current_kst_time)
    return last_completed_end - timedelta(hours=4)

def find_last_completed_candle_start_time_before_point(target_kst_time):
    """íŠ¹ì • ì‹œì  ì§ì „ì— ì™„ë£Œëœ 4ì‹œê°„ ìº”ë“¤ì˜ 'ì‹œì‘' ì‹œê°(KST)ì„ ë°˜í™˜"""
    current_hour = target_kst_time.hour
    current_minute = target_kst_time.minute
    
    # 4ì‹œê°„ ìº”ë“¤ ì™„ë£Œ ì‹œê°„: 01ì‹œ, 05ì‹œ, 09ì‹œ, 13ì‹œ, 17ì‹œ, 21ì‹œ
    # ê²½ê³„ê°’ ë¡œì§ ìˆ˜ì •: ë§Œì•½ ì…ë ¥ì´ ì •í™•íˆ 17:00ì´ë¼ë©´, 17:00ì— "ëë‚œ" ìº”ë“¤ì„ ì°¾ì•„ì•¼ í•¨ (ì‹œì‘ 13:00)
    # ê¸°ì¡´ ë¡œì§ì€ 17:00 ì…ë ¥ ì‹œ 17:00ì— "ì‹œì‘í•˜ëŠ”" ìº”ë“¤ì„ ê°€ë¦¬í‚¬ ìˆ˜ ìˆì—ˆìŒ
    
    last_completed_candle_end_hour = None
    target_date = target_kst_time.date()
    
    # ì •í™•í•œ ê²½ê³„ ì‹œê°„(ë¶„=0)ì¸ì§€ í™•ì¸
    is_exact_boundary = (current_minute == 0)
    
    if current_hour > 21 or (current_hour == 21 and is_exact_boundary):
        last_completed_candle_end_hour = 21
    elif current_hour > 17 or (current_hour == 17 and is_exact_boundary):
        last_completed_candle_end_hour = 17
    elif current_hour > 13 or (current_hour == 13 and is_exact_boundary):
        last_completed_candle_end_hour = 13
    elif current_hour > 9 or (current_hour == 9 and is_exact_boundary):
        last_completed_candle_end_hour = 9
    elif current_hour > 5 or (current_hour == 5 and is_exact_boundary):
        last_completed_candle_end_hour = 5
    elif current_hour > 1 or (current_hour == 1 and is_exact_boundary):
        last_completed_candle_end_hour = 1
    else:  
        # 01:00 ë¯¸ë§Œ ë˜ëŠ” (01:00ì´ ì•„ë‹Œ 00:XX ë“±) -> ì „ë‚  21:00 ì™„ë£Œ
        last_completed_candle_end_hour = 21  
        target_date = (target_kst_time - timedelta(days=1)).date()
    
    # ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ì˜ ì™„ë£Œ ì‹œê°„ (íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ì‹œì )
    completed_time = target_kst_time.replace(
        year=target_date.year,
        month=target_date.month, 
        day=target_date.day,
        hour=last_completed_candle_end_hour, 
        minute=0, 
        second=0, 
        microsecond=0
    )
    
    # ì‹œì‘ ì‹œê°„ = ì™„ë£Œ ì‹œê°„ - 4ì‹œê°„
    start_time = completed_time - timedelta(hours=4)
    
    print(f"ğŸ• Target KST: {target_kst_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Adjusted Start Time: {start_time.strftime('%Y-%m-%d %H:%M:%S KST')} (Ends at {completed_time.strftime('%H:%M')})")
    
    return start_time

def convert_kst_to_utc_timestamp(kst_datetime):
    """KST datetimeì„ UTC íƒ€ì„ìŠ¤íƒ¬í”„(ë°€ë¦¬ì´ˆ)ë¡œ ë³€í™˜"""
    utc_dt = kst_datetime.astimezone(pytz.UTC)
    return int(utc_dt.timestamp() * 1000)

def convert_utc_to_kst(utc_timestamp_ms):
    """UTC íƒ€ì„ìŠ¤íƒ¬í”„(ë°€ë¦¬ì´ˆ)ë¥¼ KST datetimeìœ¼ë¡œ ë³€í™˜"""
    utc_dt = datetime.fromtimestamp(utc_timestamp_ms / 1000, tz=pytz.UTC)
    return utc_dt.astimezone(KST)

def sync_binance_time():
    """Binance ì„œë²„ ì‹œê°„ ë™ê¸°í™”"""
    global binance_exchange
    try:
        if binance_exchange is None:
            print("âŒ Binance exchange not initialized")
            return False
            
        # ê¸°ì¡´ íƒ€ì„ì•„ì›ƒ ì„¤ì • ë°±ì—…
        original_timeout = getattr(binance_exchange, 'timeout', 10000)
        
        # ì„œë²„ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°ìš© ì§§ì€ íƒ€ì„ì•„ì›ƒ ì„¤ì • (5ì´ˆ)
        binance_exchange.timeout = 5000
        
        # Binance ì„œë²„ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        server_time = binance_exchange.fetch_time()
        local_time = binance_exchange.milliseconds()
        
        # ì›ë˜ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë³µì›
        binance_exchange.timeout = original_timeout
        
        # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ë°€ë¦¬ì´ˆ)
        time_offset = server_time - local_time
        
        # ì‹œê°„ ì°¨ì´ë¥¼ 0ì— ê°€ê¹ê²Œ ë³´ì • (ë°˜ëŒ€ ê°’ ì‚¬ìš©)
        safe_offset = -time_offset
        
        # ì‹œê°„ ì°¨ì´ë¥¼ ê±°ë˜ì†Œ ê°ì²´ì— ì„¤ì •
        binance_exchange.options['timeDifference'] = safe_offset
        
        print(f"ğŸ• Binance time sync: offset={time_offset}ms â†’ safe_offset={safe_offset}ms (server={server_time}, local={local_time})")
        
        # ì‹œê°„ ì°¨ì´ì— ë”°ë¥¸ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
        if abs(time_offset) > 1000:
            print(f"âš ï¸  Large time difference detected: {time_offset}ms")
        elif abs(time_offset) < 50:
            print("âœ… Time sync excellent: < 50ms difference")
            
        return True
    except Exception as e:
        print(f"âŒ Time sync failed: {e}")
        # íƒ€ì„ì•„ì›ƒ ë³µì› ì‹œë„
        try:
            if binance_exchange:
                binance_exchange.timeout = original_timeout
        except:
            pass
        return False

def initialize_binance():
    """ë°”ì´ë‚¸ìŠ¤ ê±°ë˜ì†Œ ì´ˆê¸°í™” (Public API ì‚¬ìš©)"""
    global binance_exchange
    try:
        # API í‚¤ í™•ì¸
        api_key = CONFIG['binance_api_key']
        api_secret = CONFIG['binance_api_secret']
        
        if api_key and api_secret:
            # API í‚¤ê°€ ìˆëŠ” ê²½ìš° ì¸ì¦ëœ ì—°ê²°
            binance_exchange = ccxt.binance({
                'apiKey': api_key,
                'secret': api_secret,
                'sandbox': False,
                'enableRateLimit': True,
                'options': {
                    'timeDifference': 0,
                }
            })
            print("âœ… Binance exchange initialized with API credentials")
        else:
            # API í‚¤ê°€ ì—†ëŠ” ê²½ìš° Public APIë§Œ ì‚¬ìš©
            binance_exchange = ccxt.binance({
                'sandbox': False,
                'enableRateLimit': True,
                'options': {
                    'timeDifference': 0,
                }
            })
            print("âœ… Binance exchange initialized with Public API (no credentials)")
        
        # ì´ˆê¸° ì‹œê°„ ë™ê¸°í™”
        if sync_binance_time():
            print("âœ… Initial time synchronization completed")
        else:
            print("âš ï¸  Initial time synchronization failed")
            
        return True
    except Exception as e:
        print(f"âŒ Failed to initialize Binance exchange: {e}")
        return False


def _minmax(data, epsilon=1e-8):
    """MinMax ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜ (trading ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê°€ì ¸ì˜´)"""
    min_val = torch.min(data, dim=1, keepdim=True)[0]
    max_val = torch.max(data, dim=1, keepdim=True)[0]
    range_val = max_val - min_val
    range_val = torch.where(range_val < epsilon, torch.tensor(epsilon, dtype=range_val.dtype, device=range_val.device), range_val)
    normalized = (data - min_val) / range_val
    return normalized

def safe_fetch_ohlcv(exchange, symbol, timeframe, limit=None, since=None, max_retries=3):
    """ì•ˆì „í•œ OHLCV ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ ë™ê¸°í™” ë° ì¬ì‹œë„ í¬í•¨)"""
    for attempt in range(max_retries):
        try:
            # Binance API í˜¸ì¶œ ì „ ë§¤ë²ˆ ì‹œê°„ ë™ê¸°í™”
            if hasattr(exchange, 'id') and exchange.id == 'binance':
                if not sync_binance_time():
                    print(f"âš ï¸  Time sync failed on attempt {attempt + 1}")
                    
            # API í˜¸ì¶œ
            if since is not None:
                ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=limit)
            else:
                ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
            
            print(f"âœ… Successfully fetched {len(ohlcvs)} candles on attempt {attempt + 1}")
            return ohlcvs
            
        except ccxt.NetworkError as e:
            error_msg = str(e)
            if "-1021" in error_msg or "Timestamp" in error_msg:
                print(f"ğŸ”„ Timestamp error on attempt {attempt + 1}, retrying with fresh sync...")
                if attempt < max_retries - 1:
                    # ê°•ì œ ì¬ë™ê¸°í™” ì‹œë„
                    sync_binance_time()
                    import time
                    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    continue
            raise e
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"âš ï¸  API call failed on attempt {attempt + 1}: {e}")
                import time
                time.sleep(1)
                continue
            raise e
    
    raise Exception(f"Failed to fetch OHLCV data after {max_retries} attempts")

@cached(ttl=300)  # 5ë¶„ ìºì‹œ
def fetch_and_preprocess_data(exchange, symbol, timeframe, sequence_length, scaling_method='original_minmax', epsilon=1e-8, is_auto_analysis=False):
    """OHLCV ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì„ ìœ„í•´ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. (KST ê¸°ì¤€ ì •ë ¬)"""
    try:
        # KST ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ ì™„ë£Œëœ êµ¬ê°„ì˜ ë ì‹œê°„ ì°¾ê¸°
        last_completed_end_time = find_last_completed_kst_interval()
        
        if is_auto_analysis:
            # ìë™ë¶„ì„ì€ í•œ ìº”ë“¤ ì „ ë°ì´í„° ì‚¬ìš©
            auto_analysis_end_time = last_completed_end_time - timedelta(hours=4)
            end_utc_timestamp = convert_kst_to_utc_timestamp(auto_analysis_end_time)
            print(f"ğŸ¤– Auto-analysis: Using one candle earlier - {auto_analysis_end_time.strftime('%Y-%m-%d %H:%M:%S KST')}")
        else:
            # ì¼ë°˜ ë¶„ì„ì€ ìµœì‹  ì™„ë£Œ ì‹œì  ì‚¬ìš©
            end_utc_timestamp = convert_kst_to_utc_timestamp(last_completed_end_time)
        
        # í•´ë‹¹ ì‹œì ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ëŠ” sequence_lengthê°œ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸° (ì™„ë£Œëœ ìº”ë“¤ê¹Œì§€ í¬í•¨)
        # 4ì‹œê°„ ìº”ë“¤ì´ë¯€ë¡œ 4 * 60 * 60 * 1000ms = 14400000ms per candle
        start_utc_timestamp = end_utc_timestamp - ((sequence_length - 1) * 4 * 60 * 60 * 1000)
        
        if is_auto_analysis:
            print(f"ğŸ“Š [DEBUG] Auto-analysis: Fetching {sequence_length} candles ending at KST {auto_analysis_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            print(f"ğŸ“Š [DEBUG] Fetching {sequence_length} candles ending at KST {last_completed_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Š [DEBUG] UTC timestamps: start={start_utc_timestamp}, end={end_utc_timestamp}")
        
        # ì•ˆì „í•œ OHLCV ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        ohlcvs = safe_fetch_ohlcv(exchange, symbol, timeframe, 
                                 limit=sequence_length, 
                                 since=start_utc_timestamp)
        if len(ohlcvs) < sequence_length:
            logging.warning(f"ë°ì´í„° ë¶€ì¡±: {len(ohlcvs)}ê°œ ê°€ì ¸ì˜´, {sequence_length}ê°œ í•„ìš”")
            return None, None, None

        # ê²€ì¦ì„ ìœ„í•´ ê°€ì ¸ì˜¨ ëª¨ë“  ìº”ë“¤ ë¡œê¹… (KST ì‹œê°„ìœ¼ë¡œ í‘œì‹œ)
        ohlc_strings = []
        for i, ohlc in enumerate(ohlcvs):
            kst_time = convert_utc_to_kst(ohlc[0])
            ts = kst_time.strftime('%Y-%m-%d %H:%M:%S')
            ohlc_strings.append(f"#{i+1} {ts} O:{ohlc[1]} H:{ohlc[2]} L:{ohlc[3]} C:{ohlc[4]} V:{ohlc[5]}")
        print(f"ê°€ì ¸ì˜¨ ìº”ë“¤ (ì´ {len(ohlcvs)}ê°œ, KST ì‹œê°„):\n" + "\n".join(ohlc_strings))
        print(f"ì…ë ¥ìš© ìº”ë“¤: ëª¨ë“  {sequence_length}ê°œ (#{1}-#{sequence_length}) - ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ê¹Œì§€ í¬í•¨")

        # ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ëª¨ë“  `sequence_length`ê°œì˜ ìº”ë“¤ (ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ê¹Œì§€ í¬í•¨)
        input_candles = np.array([ohlc[1:5] for ohlc in ohlcvs], dtype=np.float32)  # OHLC ë°ì´í„°ë§Œ ì‚¬ìš©

        # ìŠ¤ì¼€ì¼ë§ ë° ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ í˜•íƒœ ë³€ê²½: (1, seq_len, features)
        input_data = torch.FloatTensor(input_candles).unsqueeze(0)

        # ìŠ¤ì¼€ì¼ë§ ì ìš©
        if scaling_method == 'original_minmax':
            input_data_scaled = _minmax(input_data.clone(), epsilon)  # ì›ë³¸ ìˆ˜ì •ì„ í”¼í•˜ê¸° ìœ„í•´ clone ì‚¬ìš©
        else:
            input_data_scaled = input_data  # ìŠ¤ì¼€ì¼ë§ ì—†ìŒ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ë°©ë²•

        return input_data_scaled, ohlcvs, input_candles

    except ccxt.NetworkError as e:
        logging.error(f"OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
    except ccxt.ExchangeError as e:
        logging.error(f"OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì¤‘ ê±°ë˜ì†Œ ì˜¤ë¥˜: {e}")
    except Exception as e:
        logging.error(f"OHLCV ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    return None, None, None

def perform_pattern_search(query_normalized, query_length, top_k, target_length=None):
    """1ë‹¨ê³„ íŒ¨í„´ ê²€ìƒ‰ (Zero Padding for v8)"""
    # Renamed from perform_2_stage_pattern_search to perform_pattern_search
    # Removes Reranker and uses simple padding
    import torch.nn.functional as F
    
    print(f"ğŸ” [DEBUG] perform_pattern_search (v8) called: query_length={query_length}, top_k={top_k}, target_length={target_length}")
    print(f"ğŸ” [DEBUG] Query shape: {query_normalized.shape}")
    
    try:
        # CUDA ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ì‹œ CPU í´ë°±
        try:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
        except Exception as e:
            print(f"âš ï¸ CUDA error, falling back to CPU: {e}")
            device = torch.device('cpu')
            model.to(device)
        
        model.eval()
        with torch.no_grad():
            # ì›ë³¸ ì¿¼ë¦¬ë¥¼ í…ì„œë¡œ ë³€í™˜ (seq_len, 4) -> (1, seq_len, 4)
            query_tensor = torch.from_numpy(query_normalized.astype(np.float32)).unsqueeze(0).to(device)
            current_len = query_tensor.shape[1]
            max_len = CONFIG['max_pattern_len'] # 100

            # Zero Padding (v8 compatible)
            if current_len < max_len:
                padding = torch.zeros(1, max_len - current_len, 4, dtype=query_tensor.dtype, device=device)
                query_tensor_padded = torch.cat([query_tensor, padding], dim=1)
            else:
                query_tensor_padded = query_tensor[:, :max_len, :]
            
            print(f"ğŸ” [DEBUG] Query shape after padding: {query_tensor_padded.shape}")
            
            # Retriever ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
            query_emb = model(query_tensor_padded).cpu().numpy().flatten()
        
        # ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰ (1ë‹¨ê³„ë§Œ ìˆ˜í–‰)
        final_similarities = find_similar_patterns(
            query_emb, embedding_data, device,
            top_k=top_k, target_length=target_length, power=query_length
        )
        
        print(f"ğŸ” [DEBUG] Final results count: {len(final_similarities)}")
        print(f"ğŸ” [DEBUG] Top 5: {[(c['idx'], c['sim']) for c in final_similarities[:5]]}")
        
        return final_similarities
        
    except Exception as e:
        print(f"Error in pattern search: {e}")
        import traceback
        traceback.print_exc()
        return []

def perform_historical_pattern_search(query_time_str, query_normalized, query_length, top_k, target_length=None):
    """ê³¼ê±° ì‹œì  íŒ¨í„´ ê²€ìƒ‰ (Zero Padding for v8)"""

    try:
        # ìºì‹œ í‚¤ ìƒì„± (ì‹¤ì‹œê°„ ë¶„ì„ì€ ìºì‹œ ë¹„í™œì„±í™”)
        # current_time = datetime.now().strftime('%Y-%m-%d %H:%M') # Not used for historical
        # Cache key modified to include top_k and target_length
        cache_key = (query_time_str, query_length, target_length, top_k)
        
        # 1. ìºì‹œ í™•ì¸ (Removed specific reranker cache logic, maybe re-enable later if needed)
        # For v8, we might rely on different caching or just fast execution.
        
        # 1ë‹¨ê³„: Retrieverë¥¼ ìœ„í•´ ì¿¼ë¦¬ë¥¼ Zero Padding (v8 compatible)
        # CUDA ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ì‹œ CPU í´ë°±
        try:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
        except Exception as e:
            print(f"âš ï¸ CUDA error, falling back to CPU: {e}")
            device = torch.device('cpu')
            model.to(device)
        
        model.eval()
        with torch.no_grad():
            # ì›ë³¸ ì¿¼ë¦¬ë¥¼ í…ì„œë¡œ ë³€í™˜ (seq_len, 4) -> (1, seq_len, 4)
            query_tensor = torch.from_numpy(query_normalized.astype(np.float32)).unsqueeze(0).to(device)
            current_len = query_tensor.shape[1]
            max_len = CONFIG['max_pattern_len'] # 100

            # Zero Padding (v8 compatible)
            if current_len < max_len:
                padding = torch.zeros(1, max_len - current_len, 4, dtype=query_tensor.dtype, device=device)
                query_tensor_padded = torch.cat([query_tensor, padding], dim=1)
            else:
                query_tensor_padded = query_tensor[:, :max_len, :]
            
            print(f"ğŸ” [DEBUG] Query shape after padding: {query_tensor_padded.shape}")
            
            # Retriever ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
            query_emb = model(query_tensor_padded).cpu().numpy().flatten()
        
        # ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰ (1ë‹¨ê³„ë§Œ ìˆ˜í–‰)
        final_similarities = find_similar_patterns(
            query_emb, embedding_data, device,
            top_k=top_k, target_length=target_length, power=query_length
        )
        
        print(f"ğŸ” [DEBUG] Final results count: {len(final_similarities)}")
        print(f"ğŸ” [DEBUG] Top 5: {[(c['idx'], c['sim']) for c in final_similarities[:5]]}")
        
        return final_similarities
        
    except Exception as e:
        print(f"Error in historical pattern search: {e}")
        import traceback
        traceback.print_exc()
        return []

def convert_numpy_types(obj):
    """numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

def initialize_ai_system():
    """AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    global model, embedding_data, full_ohlc_data, full_time_data, train_ohlc_data, train_time_data
    
    try:
        print("Initializing pattern detection system...")
        
        
        # 1. ë°ì´í„° ë¡œë“œ
        full_ohlc_data, full_time_data = load_ohlc_data(CONFIG['csv_path'])
        
        # 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
        split_idx = int(len(full_ohlc_data) * 0.9)
        train_ohlc_data = full_ohlc_data[:split_idx]
        train_time_data = full_time_data[:split_idx]
        test_ohlc_data = full_ohlc_data[split_idx:]
        test_time_data = full_time_data[split_idx:]
        
        # 3. Retriever ëª¨ë¸ ë¡œë“œ
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = PatternEncoder(emb_dim=CONFIG['emb_dim'], max_len=CONFIG['max_pattern_len']).to(device)
        
        if os.path.exists(CONFIG['model_path']):
            model.load_state_dict(torch.load(CONFIG['model_path'], map_location=device))
            print(f"Retriever model loaded from {CONFIG['model_path']}")
        else:
            print("Warning: Retriever model file not found. Training required.")
            return False
        
        # 4. ì„ë² ë”© ë°ì´í„° ë¡œë“œ (Quantized)
        if os.path.exists(CONFIG['emb_path']):
            with open(CONFIG['emb_path'], 'rb') as f:
                embedding_data = pickle.load(f)
            
            # v8ì—ì„œëŠ” ì„ë² ë”©ì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ë¨ {'embeddings': ..., 'meta': ...}
            if isinstance(embedding_data, dict) and 'embeddings' in embedding_data:
                # ì„ë² ë”©ì´ ì–‘ìí™”(int8) ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œë“œ
                # find_similar_patterns í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘ 
                print(f"Embeddings loaded from {CONFIG['emb_path']} (Keys: {embedding_data.keys()})")
            else:
                 print(f"Embeddings loaded from {CONFIG['emb_path']} (Legacy format)")

        else:
            print("Warning: Embeddings file not found. Precomputing required.")
            return False
        
        # 5. ë°”ì´ë‚¸ìŠ¤ ê±°ë˜ì†Œ ì´ˆê¸°í™”
        initialize_binance()
        
        # 5. ìë™ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        global auto_scheduler
        def analysis_wrapper(**kwargs):
            """ìŠ¤ì¼€ì¤„ëŸ¬ìš© ë¶„ì„ í•¨ìˆ˜ ë˜í¼"""
            try:
                print(f"ğŸ¤– [AUTO-ANALYSIS] Starting with params: {kwargs}")
                # live_analysis_apiì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
                input_data_scaled, ohlcvs, input_candles = fetch_and_preprocess_data(
                    binance_exchange, kwargs['symbol'], kwargs['timeframe'], kwargs['query_length'], is_auto_analysis=True
                )
                
                if input_data_scaled is None:
                    return None
                
                query_normalized = normalize_window(input_candles)
                print(f"ğŸ¤– [AUTO-ANALYSIS] Input candles shape: {input_candles.shape}")
                print(f"ğŸ¤– [AUTO-ANALYSIS] Input candles first 3: {input_candles[:3] if len(input_candles) >= 3 else input_candles}")
                print(f"ğŸ¤– [AUTO-ANALYSIS] Normalized shape: {query_normalized.shape}")
                print(f"ğŸ¤– [AUTO-ANALYSIS] Normalized first 3: {query_normalized[:3] if len(query_normalized) >= 3 else query_normalized}")
                
                similarities = perform_pattern_search(
                    query_normalized, kwargs['query_length'], kwargs['top_k'], kwargs.get('target_length')
                )
                
                print(f"ğŸ¤– [AUTO-ANALYSIS] Final similarities count: {len(similarities)}")
                for i, sim in enumerate(similarities[:3]):
                    print(f"  #{i+1}: idx={sim['idx']}, sim={sim['sim']:.6f}, len={sim['len']}")
                
                # ê²°ê³¼ í¬ë§·íŒ…
                result_patterns = []
                for sim_info in similarities:
                    pattern_start_idx = sim_info['idx']
                    pattern_length = sim_info['len']
                    forecast_length = math.ceil(pattern_length / 3)
                    
                    pattern_data = convert_numpy_types(full_ohlc_data[pattern_start_idx:pattern_start_idx + pattern_length])
                    # íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œì  (ë§ˆì§€ë§‰ ìº”ë“¤ì˜ ì‹œì‘ ì‹œì )
                    pattern_start_time = full_time_data.iloc[pattern_start_idx + pattern_length - 1]
                    
                    forecast_start_idx = pattern_start_idx + pattern_length
                    forecast_end_idx = forecast_start_idx + forecast_length
                    forecast_data = None
                    
                    if forecast_end_idx <= len(full_ohlc_data):
                        forecast_data = convert_numpy_types(full_ohlc_data[forecast_start_idx:forecast_end_idx])
                    
                    # CSV ì‹œê°„ì´ 4ì‹œê°„ ê°„ê²©ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜¬ë°”ë¥¸ 4ì‹œê°„ ì™„ë£Œ ì‹œì ìœ¼ë¡œ ì •ê·œí™”
                    # íŒ¨í„´ ì‹œì‘ ì‹œì ì—ì„œ 4ì‹œê°„ì„ ë”í•´ ì™„ë£Œ ì‹œì  ê³„ì‚°
                    pattern_completion_time = pattern_start_time + timedelta(hours=4)
                    formatted_pattern = pattern_to_frontend_format(
                        sim_info, pattern_data, pattern_completion_time, forecast_data
                    )
                    if formatted_pattern:
                        result_patterns.append(formatted_pattern)
                
                # ì¿¼ë¦¬ íŒ¨í„´ ì •ë³´ ìƒì„± (ê²ŒìŠ¤íŠ¸ UI í‘œì‹œìš©)
                query_candles = []
                for ohlc in input_candles:
                    query_candles.append({
                        'open': float(ohlc[0]),
                        'high': float(ohlc[1]),
                        'low': float(ohlc[2]),
                        'close': float(ohlc[3]),
                        'volume': 1000000  # ì„ì‹œ ë³¼ë¥¨
                    })
                
                # ìë™ë¶„ì„ì€ í•œ ìº”ë“¤ ì „ ì‹œì  ì‚¬ìš©
                # í˜„ì¬ ì™„ë£Œëœ ìº”ë“¤ì—ì„œ 4ì‹œê°„(1ìº”ë“¤) ì „ ì‹œì ìœ¼ë¡œ ê³„ì‚°
                current_completed_time = find_last_completed_candle_time()
                auto_analysis_time = current_completed_time - timedelta(hours=4)
                query_pattern = {
                    'timestamp': auto_analysis_time.strftime('%Y.%m.%d %H:%M'),
                    'symbol': kwargs['symbol'],
                    'confidence': 95,
                    'candles': query_candles,
                    'source': 'auto_analysis'
                }

                return {
                    'query_pattern': query_pattern,
                    'similar_patterns': result_patterns,
                    'debug_info': {
                        'analysis_type': 'auto_analysis',
                        'candle_count': len(ohlcvs) if ohlcvs else 0,
                        'first_candle_time': convert_utc_to_kst(ohlcvs[0][0]).strftime('%Y-%m-%d %H:%M:%S KST') if ohlcvs and len(ohlcvs) > 0 else None,
                        'last_candle_time': convert_utc_to_kst(ohlcvs[-1][0]).strftime('%Y-%m-%d %H:%M:%S KST') if ohlcvs and len(ohlcvs) > 0 else None,
                        'input_candles_shape': input_candles.shape if input_candles is not None else None,
                        'normalized_data_sample': query_normalized[:2].tolist() if query_normalized is not None and len(query_normalized) >= 2 else None,
                        'similarities_raw': [{'idx': s['idx'], 'sim': float(s['sim']), 'len': s['len']} for s in similarities[:3]] if similarities else []
                    }
                }
                
            except Exception as e:
                print(f"Auto analysis wrapper error: {e}")
                return None
        
        auto_scheduler = AutoAnalysisScheduler(analysis_wrapper)
        auto_scheduler.start()
        
        print("System initialization completed successfully!")
        return True
        
    except Exception as e:
        print(f"Error during initialization: {e}")
        return False

def get_current_pattern():
    """í˜„ì¬ ì‹œì ì˜ íŒ¨í„´ ë°ì´í„° ìƒì„± (ìµœì‹  5ê°œ ìº”ë“¤)"""
    try:
        # ìµœì‹  5ê°œ ìº”ë“¤ ì‚¬ìš©
        current_raw = full_ohlc_data[-5:]
        current_normalized = normalize_window(current_raw)
        
        # í˜„ì¬ ì‹œê°„ (ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ì‹œê°„)
        current_time = full_time_data.iloc[-1]
        
        # ìº”ë“¤ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        candles = []
        for ohlc in current_raw:
            candles.append({
                'open': float(ohlc[0]),
                'high': float(ohlc[1]),
                'low': float(ohlc[2]),
                'close': float(ohlc[3]),
                'volume': 1000000  # ì„ì‹œ ë³¼ë¥¨ ë°ì´í„°
            })
        
        return {
            'timestamp': current_time.strftime("%Y.%m.%d %H:%M"),
            'symbol': 'BTC/USDT',
            'confidence': 92,  # ì„ì‹œ ì‹ ë¢°ë„
            'candles': candles,
            'normalized': current_normalized.tolist()
        }
        
    except Exception as e:
        print(f"Error getting current pattern: {e}")
        return None

def pattern_to_frontend_format(pattern_info, pattern_data, time_info, forecast_data=None):
    """íŒ¨í„´ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # í•œêµ­ì‹œê°„ ë³€í™˜ (UTC+9)
        kst = pytz.timezone('Asia/Seoul')
        if hasattr(time_info, 'tz_localize'):
            # pandas datetimeì¸ ê²½ìš°
            if time_info.tz is None:
                time_info_kst = time_info.tz_localize('UTC').tz_convert(kst)
            else:
                time_info_kst = time_info.tz_convert(kst)
        else:
            # ì¼ë°˜ datetimeì¸ ê²½ìš°
            if time_info.tzinfo is None:
                time_info_kst = pytz.UTC.localize(time_info).astimezone(kst)
            else:
                time_info_kst = time_info.astimezone(kst)
        # íŒ¨í„´ ìº”ë“¤ ë°ì´í„°
        pattern_candles = []
        for ohlc in pattern_data:
            pattern_candles.append({
                'open': float(ohlc[0]),
                'high': float(ohlc[1]),
                'low': float(ohlc[2]),
                'close': float(ohlc[3]),
                # 'volume': int(np.random.randint(500000, 2000000)),  # ì„ì‹œ ë³¼ë¥¨
                'volume': 1000000,  # ì„ì‹œ ë³¼ë¥¨ (ëœë¤ ê°’ ì œê±°)
                'type': 'pattern'  # íŒ¨í„´ êµ¬ê°„ í‘œì‹œ
            })
        
        # ë¯¸ë˜ ìº”ë“¤ ë°ì´í„° (ì˜ˆì¸¡ êµ¬ê°„)
        forecast_candles = []
        if forecast_data is not None and len(forecast_data) > 0:
            for ohlc in forecast_data:
                forecast_candles.append({
                    'open': float(ohlc[0]),
                    'high': float(ohlc[1]),
                    'low': float(ohlc[2]),
                    'close': float(ohlc[3]),
                    # 'volume': int(np.random.randint(500000, 2000000)),  # ì„ì‹œ ë³¼ë¥¨
                    'volume': 1000000,  # ì„ì‹œ ë³¼ë¥¨ (ëœë¤ ê°’ ì œê±°)
                    'type': 'forecast'  # ì˜ˆì¸¡ êµ¬ê°„ í‘œì‹œ
                })
            
            # ì‹¤ì œ ë¯¸ë˜ ê°€ê²© ë³€í™” ê³„ì‚° (íŒ¨í„´ ë§ˆì§€ë§‰ ì¢…ê°€ ëŒ€ë¹„ ì˜ˆìƒ êµ¬ê°„ ìµœëŒ€ ìƒìŠ¹/í•˜ë½)
            if len(forecast_data) > 0:
                pattern_close = pattern_data[-1][3]  # íŒ¨í„´ ë§ˆì§€ë§‰ ì¢…ê°€
                
                # ì˜ˆìƒ êµ¬ê°„ì˜ ëª¨ë“  ìº”ë“¤ì—ì„œ ìµœê³ ê°€ì™€ ìµœì €ê°€ ì°¾ê¸°
                forecast_highs = [candle[1] for candle in forecast_data]  # ëª¨ë“  high ê°’
                forecast_lows = [candle[2] for candle in forecast_data]   # ëª¨ë“  low ê°’
                
                max_high = max(forecast_highs)  # ì˜ˆìƒ êµ¬ê°„ ìµœëŒ€ ê³ ê°€
                min_low = min(forecast_lows)    # ì˜ˆìƒ êµ¬ê°„ ìµœì†Œ ì €ê°€
                
                # íŒ¨í„´ ë§ˆì§€ë§‰ ì¢…ê°€ ëŒ€ë¹„ ìµœëŒ€ ìƒìŠ¹/í•˜ë½ë¥  ê³„ì‚°
                max_rise = ((max_high - pattern_close) / pattern_close) * 100    # ìµœëŒ€ ìƒìŠ¹
                max_fall = ((min_low - pattern_close) / pattern_close) * 100     # ìµœëŒ€ í•˜ë½ (ìŒìˆ˜)
                
                price_change_7d = round(max_rise, 1)   # ìµœëŒ€ ìƒìŠ¹ë¥ 
                price_change_3d = round(max_fall, 1)   # ìµœëŒ€ í•˜ë½ë¥ 
            else:
                price_change_7d = 0.0  # ìµœëŒ€ ìƒìŠ¹
                price_change_3d = 0.0  # ìµœëŒ€ í•˜ë½
        else:
            # # ë¯¸ë˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì„ì‹œ ê°’
            # price_change_7d = float(np.random.uniform(-20, 20))
            # price_change_3d = float(np.random.uniform(-15, 15))
            # ë¯¸ë˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì„ì‹œ ê°’ (ëœë¤ ê°’ ì œê±°)
            price_change_7d = 0.0
            price_change_3d = 0.0
        
        # ì „ì²´ ìº”ë“¤ ë°ì´í„° (íŒ¨í„´ + ì˜ˆì¸¡)
        all_candles = pattern_candles + forecast_candles
        
        # pattern_detecter.pyì—ì„œ ì´ë¯¸ ì‹¤ì œ similarity ê³µì‹ì´ ì ìš©ë˜ì–´ ì „ë‹¬ë¨
        # power = round(200 / power) if round(100 / power) % 2 == 1 else round(100 / power) + 1
        # sim = np.sign(cosine_similarity) * np.power(np.abs(cosine_similarity), power)

        # âœ… ì‹œê°„ ì •ì±…: time_infoë¥¼ ë§ˆì§€ë§‰ ìº”ë“¤ì˜ ì‹œì‘ ì‹œê°ìœ¼ë¡œ ì§ì ‘ ì‚¬ìš©
        hour = time_info_kst.hour
        
        # 4ì‹œê°„ ìº”ë“¤ì˜ ì‹œì‘ ì‹œê°ë“¤: 1, 5, 9, 13, 17, 21
        start_hours = {1, 5, 9, 13, 17, 21}
        
        if hour in start_hours:
            # time_infoê°€ ì´ë¯¸ ì‹œì‘ ì‹œê°ì¸ ê²½ìš°
            ts_start = time_info_kst
            ts_complete = time_info_kst + timedelta(hours=4)
        else:
            # time_infoê°€ ì™„ë£Œ ì‹œê°ì´ë‚˜ ê¸°íƒ€ ì‹œê°ì¸ ê²½ìš° â†’ 4ì‹œê°„ ê·¸ë¦¬ë“œì— ë§ì¶¤
            # ê°€ì¥ ê°€ê¹Œìš´ ì´ì „ ì‹œì‘ ì‹œê°ì„ ì°¾ê¸°
            anchors = [1, 5, 9, 13, 17, 21]
            same_day_anchors = [time_info_kst.replace(hour=h, minute=0, second=0, microsecond=0) for h in anchors]
            prev_day_21 = (time_info_kst - timedelta(days=1)).replace(hour=21, minute=0, second=0, microsecond=0)
            
            # time_info ì´ì „ ë˜ëŠ” ê°™ì€ ì‹œê°ì˜ ì‹œì‘ì ë“¤ë§Œ ê³ ë ¤
            candidates = [a for a in same_day_anchors if a <= time_info_kst] + [prev_day_21]
            ts_start = max(candidates) if candidates else time_info_kst.replace(hour=21, minute=0, second=0, microsecond=0) - timedelta(days=1)
            ts_complete = ts_start + timedelta(hours=4)

        # ì‹œì  í‘œì‹œìš©ìœ¼ë¡œ 8ì‹œê°„ ë¹¼ê¸°
        display_time = ts_start - timedelta(hours=8)
        display_complete = ts_complete - timedelta(hours=8)
        
        return {
            'id': int(pattern_info['idx']),
            # âœ… í˜¸í™˜ì„±: timestamp = ì‹œì‘ ì‹œê° (8ì‹œê°„ ëº€ ê°’)
            'timestamp': display_time.strftime("%Y.%m.%d %H:%M"),
            # âœ… ëª…ì‹œì ìœ¼ë¡œ ë‘˜ ë‹¤ ì œê³µ (8ì‹œê°„ ëº€ ê°’)
            'timestamp_start': display_time.strftime("%Y.%m.%d %H:%M"),
            'timestamp_complete': display_complete.strftime("%Y.%m.%d %H:%M"),
            'similarity': float(pattern_info['sim']),
            'priceChange7d': price_change_7d,
            'priceChange3d': price_change_3d,
            'candles': all_candles,
            'pattern_length': len(pattern_candles),
            'forecast_length': len(forecast_candles),
            'has_forecast': len(forecast_candles) > 0
        }
        
    except Exception as e:
        print(f"Error converting pattern to frontend format: {e}")
        return None

@app.route('/api/health', methods=['GET'])
def health_check():
    """API ìƒíƒœ í™•ì¸
    ---
    tags:
      - System
    responses:
      200:
        description: API and system component status
        schema:
            type: object
            properties:
                status:
                    type: string
                    example: ok
                model_loaded:
                    type: boolean
                binance_connected:
                    type: boolean
    """
    next_analysis = auto_scheduler.get_next_analysis_time() if auto_scheduler else None
    last_analysis = auto_scheduler.get_last_analysis_time() if auto_scheduler else None
    
    return jsonify({
        'status': 'ok',
        'model_loaded': model is not None,
        'embeddings_loaded': embedding_data is not None,
        'cache_loaded': False, # Legacy cache removed
        'cache_size': 0,
        'data_loaded': full_ohlc_data is not None,
        'binance_connected': binance_exchange is not None,
        'auto_scheduler_running': auto_scheduler.is_running if auto_scheduler else False,
        'next_auto_analysis': next_analysis,
        'last_auto_analysis': last_analysis
    })

@app.route('/api/auto-analysis', methods=['GET'])
@require_auth(min_tier=UserTier.GUEST.value)
def get_auto_analysis_api():
    """ìë™ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (ê²ŒìŠ¤íŠ¸ ì´ìƒ)
    ---
    tags:
      - Analysis
    parameters:
      - name: Authorization
        in: header
        type: string
        required: true
        description: Bearer token
    responses:
      200:
        description: Cached auto-analysis results
        schema:
          type: object
          properties:
            results:
              type: object
              description: Analysis results including query and similar patterns
            analysis_time:
              type: string
              description: ISO timestamp of analysis
    """
    try:
        # ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
        user_tier = request.user_info['tier']
        limits = get_tier_limits(user_tier)
        
        # ê²ŒìŠ¤íŠ¸ëŠ” 3:3, top_k=3 ì¡°í•© ì¡°íšŒ
        if user_tier == 'guest':
            query_length = 3
            target_length = 3
            top_k = 3
        else:
            # ë©¤ë²„/í”„ë¦¬ë¯¸ì—„ì€ ê¸°ë³¸ 3:3 ì¡°í•©ìœ¼ë¡œ ì¡°íšŒ (ë” ë§ì€ ê²°ê³¼)
            query_length = 3
            target_length = 3  
            top_k = limits['max_top_k']
        
        # ğŸ¯ ê°„ë‹¨í•˜ê²Œ: ê°€ì¥ ìµœê·¼ ìë™ë¶„ì„ ìºì‹œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        print(f"ğŸ” Looking for most recent auto analysis cache (q{query_length}:t{target_length}, top{top_k})")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì¥ ìµœì‹  ìºì‹œ ê°€ì ¸ì˜¤ê¸° (top_këŠ” >=ë¡œ ë¹„êµí•˜ë¯€ë¡œ ìºì‹œì— ë” ë§ì€ ê²°ê³¼ê°€ ìˆì–´ë„ OK)
        results, analysis_time = db_manager.get_latest_auto_analysis(
            symbol="BTC/USDT", 
            timeframe="4h", 
            query_length=query_length, 
            target_length=target_length, 
            top_k=top_k  # ìºì‹œì— top_k >= ì´ ê°’ì¸ ê²°ê³¼ ì°¾ê¸°
        )
        
        if results and analysis_time:
            print(f"âœ… Found cached analysis from: {analysis_time}")
            result = (json.dumps(results), analysis_time)
        else:
            print("âŒ No matching cache found")
            result = None
        
        if result:
            results = json.loads(result[0])
            analysis_time = result[1]
            print(f"âœ… Found most recent cached analysis from {analysis_time}")
            
            # ğŸ¯ ê²ŒìŠ¤íŠ¸/íšŒì›ìš©ìœ¼ë¡œ ê²°ê³¼ ì œí•œ
            if results.get('similar_patterns') and len(results['similar_patterns']) > top_k:
                results['similar_patterns'] = results['similar_patterns'][:top_k]
                print(f"    Limited results to top {top_k} for {user_tier}")
            
            # âœ… ìºì‹œëœ ì¿¼ë¦¬ íŒ¨í„´ì˜ ì‹œê°„ ì •ë³´ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # pattern_to_frontend_format()ì—ì„œ ì´ë¯¸ 8ì‹œê°„ ë¹¼ê¸°ê°€ ì ìš©ë˜ì–´ ìˆìŒ
        else:
            print(f"âŒ No cached analysis found, trying live analysis...")
            # ìºì‹œì— ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ ë¶„ì„ ì‹¤í–‰
            try:
                # auto analysisìš© í•œ ìº”ë“¤ ì „ ë°ì´í„°ë¡œ ì‹¤ì‹œê°„ ë¶„ì„
                input_data_scaled, ohlcvs, input_candles = fetch_and_preprocess_data(
                    binance_exchange, "BTC/USDT", "4h", query_length, is_auto_analysis=True
                )
                
                if input_data_scaled is not None:
                    query_normalized = normalize_window(input_candles)
                    similarities = perform_pattern_search(
                        query_normalized, query_length, top_k, target_length
                    )
                    
                    # ê²°ê³¼ í¬ë§·íŒ… (auto analysisì™€ ë™ì¼í•œ ë°©ì‹)
                    result_patterns = []
                    for sim_info in similarities:
                        pattern_start_idx = sim_info['idx']
                        pattern_length = sim_info['len']
                        forecast_length = math.ceil(pattern_length / 3)
                        
                        pattern_data = convert_numpy_types(full_ohlc_data[pattern_start_idx:pattern_start_idx + pattern_length])
                        # íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œì  (CSVì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
                        pattern_completion_time = full_time_data.iloc[pattern_start_idx + pattern_length - 1]
                        
                        forecast_start_idx = pattern_start_idx + pattern_length
                        forecast_end_idx = forecast_start_idx + forecast_length
                        forecast_data = None
                        
                        if forecast_end_idx <= len(full_ohlc_data):
                            forecast_data = convert_numpy_types(full_ohlc_data[forecast_start_idx:forecast_end_idx])
                        
                        formatted_pattern = pattern_to_frontend_format(
                            sim_info, pattern_data, pattern_completion_time, forecast_data
                        )
                        if formatted_pattern:
                            result_patterns.append(formatted_pattern)
                    
                    # ì¿¼ë¦¬ íŒ¨í„´ ì •ë³´ ìƒì„±
                    query_candles = []
                    for ohlc in input_candles:
                        query_candles.append({
                            'open': float(ohlc[0]),
                            'high': float(ohlc[1]),
                            'low': float(ohlc[2]),
                            'close': float(ohlc[3]),
                            'volume': 1000000
                        })
                    
                    # âœ… ì˜¬ë°”ë¥¸ ì‹œê°„ ê¸°ì  ì‚¬ìš© (ì¿¼ë¦¬ íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œê°„)
                    # ìë™ ë¶„ì„ì€ í•œ ìº”ë“¤ ì „ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, í•´ë‹¹ ì‹œì ì˜ ì™„ë£Œ ì‹œê°„ì„ ê³„ì‚°
                    auto_analysis_time = get_current_kst_time() - timedelta(hours=4)
                    pattern_completion_time = find_last_completed_candle_time(auto_analysis_time)
                    
                    # 8ì‹œê°„ ë¹¼ê¸° ì ìš© (display_time)
                    display_time = pattern_completion_time - timedelta(hours=8)
                    pattern_start_time = pattern_completion_time - timedelta(hours=4*query_length)
                    display_start_time = pattern_start_time - timedelta(hours=8)
                    
                    query_pattern = {
                        'timestamp': display_time.strftime('%Y.%m.%d %H:%M'),
                        'timestamp_start': display_start_time.strftime('%Y.%m.%d %H:%M'),
                        'timestamp_complete': display_time.strftime('%Y.%m.%d %H:%M'),
                        'symbol': "BTC/USDT",
                        'confidence': 95,
                        'candles': query_candles,
                        'source': 'live_analysis'
                    }
                    
                    results = {
                        'query_pattern': query_pattern,
                        'similar_patterns': result_patterns
                    }
                    analysis_time = datetime.now().isoformat()
                    
                    print(f"âœ… Live analysis completed for {query_pattern['timestamp']}")
                else:
                    print("âŒ Failed to fetch live data")
            except Exception as e:
                print(f"âŒ Live analysis error: {e}")
        
        if not results:
            return jsonify({'error': 'No analysis results available'}), 404
        
        # ê²°ê³¼ ê°œìˆ˜ ì œí•œ (ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
        if 'similar_patterns' in results:
            max_results = limits['max_top_k']
            results['similar_patterns'] = results['similar_patterns'][:max_results]
        
        return jsonify({
            'analysis_time': analysis_time,
            'results': results,
            'user_tier': user_tier,
            'limits': limits,
            'query_config': {
                'query_length': query_length,
                'target_length': target_length,
                'top_k': top_k
            }
        })
        
    except Exception as e:
        print(f"Error in auto analysis API: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/current-pattern', methods=['GET'])
def get_current_pattern_api():
    """í˜„ì¬ íŒ¨í„´ ì •ë³´ ë°˜í™˜"""
    if model is None or embedding_data is None:
        return jsonify({'error': 'System not initialized'}), 500
    
    current_pattern = get_current_pattern()
    if current_pattern is None:
        return jsonify({'error': 'Failed to get current pattern'}), 500
    
    return jsonify(current_pattern)

@app.route('/api/latest-time', methods=['GET'])
def get_latest_time_api():
    """ê°€ì¥ ìµœê·¼ì— ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ ë°˜í™˜
    ---
    tags:
      - System
    description: Returns the latest completed candle time in KST
    responses:
      200:
        description: Latest candle timestamp information
        schema:
            type: object
            properties:
                latest_time:
                    type: string
                    example: "2024-12-12T17:00"
                kst_time:
                    type: string
                timestamp:
                    type: string
    """
    try:
        # ê°€ì¥ ìµœê·¼ì— ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ (KST) ê°€ì ¸ì˜¤ê¸°
        latest_completed_time = find_last_completed_candle_time()
        
        # datetime-local í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (YYYY-MM-DDTHH:MM)
        formatted_time = latest_completed_time.strftime('%Y-%m-%dT%H:%M')
        
        return jsonify({
            'latest_time': formatted_time,
            'kst_time': latest_completed_time.strftime('%Y-%m-%d %H:%M:%S KST'),
            'timestamp': latest_completed_time.isoformat()
        })
    except Exception as e:
        print(f"Error getting latest time: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/live-analysis', methods=['POST'])
@require_auth(min_tier=UserTier.GUEST.value)
def live_analysis_api():
    """ì‹¤ì‹œê°„ ë°”ì´ë‚¸ìŠ¤ ë°ì´í„°ë¡œ íŒ¨í„´ ë¶„ì„ (ê²ŒìŠ¤íŠ¸ ì´ìƒ)"""
    if model is None or embedding_data is None:
        return jsonify({'error': 'System not initialized'}), 500
    
    if binance_exchange is None:
        return jsonify({'error': 'Binance exchange not initialized'}), 500
    
    try:
        data = request.get_json()
        symbol = data.get('symbol', 'BTC/USDT')
        timeframe = data.get('timeframe', '4h')
        query_length = data.get('query_length', 5)
        target_length = data.get('target_length')  # Noneì´ë©´ ëª¨ë“  ê¸¸ì´
        top_k = data.get('top_k', 3)
        custom_candles = data.get('custom_candles')  # ì»¤ìŠ¤í…€ ìº”ë“¤ ë°ì´í„°
        
        print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Live analysis parameters:")
        print(f"  Symbol: {symbol}")
        print(f"  Timeframe: {timeframe}")
        print(f"  Query length: {query_length}")
        print(f"  Target length: {target_length}")
        print(f"  Top K: {top_k}")
        print(f"  Custom candles: {'Yes' if custom_candles else 'No'}")
        
        # ğŸ¯ ì»¤ìŠ¤í…€ ìº”ë“¤ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìºì‹œ í™•ì¸
        if not custom_candles:
            print(f"ğŸ” [CACHE-CHECK] Checking cache for live analysis...")
            cache_results, cache_time = db_manager.get_latest_auto_analysis(
                symbol=symbol, 
                timeframe=timeframe, 
                query_length=query_length, 
                target_length=target_length if target_length else 3,  # Noneì¼ ë•Œ ê¸°ë³¸ê°’
                top_k=top_k
            )
            
            if cache_results and cache_results.get('similar_patterns'):
                print(f"âœ… [CACHE-HIT] Found cached results from {cache_time}")
                print(f"ğŸ“Š [CACHE-HIT] {len(cache_results['similar_patterns'])} patterns in cache")
                
                # ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
                return jsonify({
                    'success': True,
                    'query_pattern': cache_results.get('query_pattern'),
                    'similar_patterns': cache_results.get('similar_patterns'),
                    'live_data_info': cache_results.get('live_data_info'),
                    'source': 'cache',
                    'cache_time': cache_time
                })
            else:
                print(f"âŒ [CACHE-MISS] No cache found, proceeding with live analysis...")
        
        if custom_candles:
            print(f"ğŸ¯ [CUSTOM-ANALYSIS] Starting custom candle analysis")
            print(f"  Custom candles data: {len(custom_candles)} candles")
            
            # ì»¤ìŠ¤í…€ ìº”ë“¤ ë°ì´í„° ì²˜ë¦¬
            # import numpy as np
            
            # ì»¤ìŠ¤í…€ ìº”ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            input_candles = np.array(custom_candles, dtype=np.float32)
            print(f"ğŸ¯ [CUSTOM-ANALYSIS] Input candles shape: {input_candles.shape}")
            print(f"ğŸ¯ [CUSTOM-ANALYSIS] Input candles: {input_candles}")
            
            # ì •ê·œí™” (íŒ¨í„´ ê²€ì¶œìš©)
            query_normalized = normalize_window(input_candles)
            print(f"ğŸ¯ [CUSTOM-ANALYSIS] Normalized shape: {query_normalized.shape}")
            print(f"ğŸ¯ [CUSTOM-ANALYSIS] Normalized first 3: {query_normalized[:3] if len(query_normalized) >= 3 else query_normalized}")
            
            # ë¶„ì„ì„ ìœ„í•œ ê°€ì§œ ohlcvs ë°ì´í„° ìƒì„± (ì°¨íŠ¸ í‘œì‹œìš©)
            ohlcvs = []
            for i, candle in enumerate(input_candles):
                ohlcvs.append({
                    'time': int((pd.Timestamp.now() - pd.Timedelta(hours=(len(input_candles) - i))).timestamp()),
                    'open': float(candle[0]),
                    'high': float(candle[1]),
                    'low': float(candle[2]),
                    'close': float(candle[3]),
                    'volume': 1000.0  # ì„ì‹œ ë³¼ë¥¨
                })
        else:
            print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Starting live analysis for {symbol} on {timeframe} timeframe")
            
            # 1. ë°”ì´ë‚¸ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            input_data_scaled, ohlcvs, input_candles = fetch_and_preprocess_data(
                binance_exchange, symbol, timeframe, query_length
            )
            
            if input_data_scaled is None:
                return jsonify({'error': 'Failed to fetch live data from Binance'}), 500
            
            # 2. ì •ê·œí™” (íŒ¨í„´ ê²€ì¶œìš©)
            query_normalized = normalize_window(input_candles)
            print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Input candles shape: {input_candles.shape}")
            print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Input candles first 3: {input_candles[:3] if len(input_candles) >= 3 else input_candles}")
            print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Normalized shape: {query_normalized.shape}")
            print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Normalized first 3: {query_normalized[:3] if len(query_normalized) >= 3 else query_normalized}")
        
        # ê³µí†µ: íŒ¨í„´ ê²€ìƒ‰ (Reranker ì œê±°ë¨)
        similarities = perform_pattern_search(
            query_normalized, query_length, top_k, target_length
        )
        
        print(f"ğŸ‘¤ [MANUAL-ANALYSIS] Final similarities count: {len(similarities)}")
        for i, sim in enumerate(similarities[:3]):
            print(f"  #{i+1}: idx={sim['idx']}, sim={sim['sim']:.6f}, len={sim['len']}")
        
        # numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        similarities = convert_numpy_types(similarities)
        
        print(f"Converted similarities: {similarities}")
        
        # 5. í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result_patterns = []
        for sim_info in similarities:
            try:
                pattern_start_idx = sim_info['idx']
                pattern_length = sim_info['len']
                forecast_length = math.ceil(pattern_length / 3)  # íŒ¨í„´ ê¸¸ì´ì˜ 1/3ë§Œí¼ ë¯¸ë˜ ì˜ˆì¸¡
                
                if pattern_start_idx + pattern_length > len(full_time_data):
                    continue

                # íŒ¨í„´ ë°ì´í„° (numpy ë°°ì—´ì„ Python listë¡œ ë³€í™˜)
                pattern_data = full_ohlc_data[pattern_start_idx:pattern_start_idx + pattern_length]
                pattern_data = convert_numpy_types(pattern_data)
                # íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œì  (CSVì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
                pattern_completion_time = full_time_data.iloc[pattern_start_idx + pattern_length - 1]
                
                # ë¯¸ë˜ ë°ì´í„° (ì˜ˆì¸¡ êµ¬ê°„)
                forecast_start_idx = pattern_start_idx + pattern_length
                forecast_end_idx = forecast_start_idx + forecast_length
                forecast_data = None
                
                if forecast_end_idx <= len(full_ohlc_data):
                    forecast_data = full_ohlc_data[forecast_start_idx:forecast_end_idx]
                    forecast_data = convert_numpy_types(forecast_data)
                
                formatted_pattern = pattern_to_frontend_format(
                    sim_info, pattern_data, pattern_completion_time, forecast_data
                )
                if formatted_pattern:
                    result_patterns.append(formatted_pattern)
            except Exception as e:
                print(f"âš ï¸ Error processing similarity result {sim_info}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # 6. í˜„ì¬ íŒ¨í„´ ì •ë³´ ìƒì„±
        candles = []
        for ohlc in input_candles:
            candles.append({
                'open': float(ohlc[0]),
                'high': float(ohlc[1]),
                'low': float(ohlc[2]),
                'close': float(ohlc[3]),
                'volume': 1000000  # ì„ì‹œ ë³¼ë¥¨
            })
        
        current_time = datetime.now().strftime("%Y.%m.%d %H:%M")
        # confidence = int(85 + np.random.randint(-10, 15))  # ì„ì‹œ ì‹ ë¢°ë„
        confidence = 85  # ì„ì‹œ ì‹ ë¢°ë„ (ëœë¤ ê°’ ì œê±°)
        
        live_pattern = {
            'timestamp': current_time,
            'symbol': symbol,
            'confidence': confidence,
            'candles': candles,
            'source': 'live_binance'
        }
        
        result = {
            'query_pattern': live_pattern,
            'similar_patterns': result_patterns,
            'live_data_info': {
                'symbol': symbol,
                'timeframe': timeframe,
                'query_length': query_length,
                'fetched_candles': len(ohlcvs),
                'used_candles': len(input_candles),
                'latest_candle_time': datetime.fromtimestamp(ohlcvs[-1][0] / 1000).strftime('%Y-%m-%d %H:%M:%S'),
                'analysis_candles_time_range': f"{datetime.fromtimestamp(ohlcvs[0][0] / 1000).strftime('%Y-%m-%d %H:%M')} ~ {datetime.fromtimestamp(ohlcvs[query_length-1][0] / 1000).strftime('%Y-%m-%d %H:%M')}",
                'data_source': 'Binance API'
            },
            'debug_info': {
                'analysis_type': 'manual_analysis',
                'candle_count': len(ohlcvs),
                'first_candle_time': convert_utc_to_kst(ohlcvs[0][0]).strftime('%Y-%m-%d %H:%M:%S KST'),
                'last_candle_time': convert_utc_to_kst(ohlcvs[-1][0]).strftime('%Y-%m-%d %H:%M:%S KST'),
                'input_candles_shape': input_candles.shape,
                'normalized_data_sample': query_normalized[:2].tolist() if len(query_normalized) >= 2 else query_normalized.tolist()
            }
        }
        
        # ìµœì¢… ì•ˆì „ì¥ì¹˜: ëª¨ë“  numpy íƒ€ì… ë³€í™˜
        result = convert_numpy_types(result)
        
        # ğŸ¯ ì»¤ìŠ¤í…€ ìº”ë“¤ì´ ì•„ë‹ˆê³  ìºì‹œ ë¯¸ìŠ¤ì˜€ë‹¤ë©´ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        if not custom_candles and 'cache_time' not in result:
            try:
                # from datetime import datetime
                import pytz
                KST = pytz.timezone('Asia/Seoul')
                current_time_kst = datetime.now(KST)
                
                print(f"ğŸ’¾ [CACHE-SAVE] Saving analysis results to cache...")
                db_manager.cache_auto_analysis(
                    analysis_time=current_time_kst,
                    symbol=symbol,
                    timeframe=timeframe,
                    query_length=query_length,
                    target_length=target_length if target_length else 3,
                    top_k=top_k,
                    results=result
                )
                print(f"âœ… [CACHE-SAVE] Results cached successfully")
                result['source'] = 'live_analysis_cached'
            except Exception as cache_error:
                print(f"âš ï¸ [CACHE-SAVE] Failed to cache results: {cache_error}")
                result['source'] = 'live_analysis'
        
        return jsonify(result)
        
    except Exception as e:
        print(f"Error in live analysis: {e}")
        return jsonify({'error': str(e)}), 500

@cached(ttl=180)  # 3ë¶„ ìºì‹œ
def _find_similar_patterns_cached(query_data_str, query_length, top_k, target_length):
    """ìºì‹œëœ ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰ í—¬í¼"""
    try:
        query_normalized = np.fromstring(query_data_str, sep=',').reshape(-1, 4)
        
        # ëª¨ë¸ ì¶”ë¡ 
        device = next(model.parameters()).device
        query_tensor = torch.tensor(query_normalized, dtype=torch.float32)
        query_tensor_resized = query_tensor.unsqueeze(0).to(device)
        
        with torch.no_grad():
            query_emb = model(query_tensor_resized).cpu().numpy().flatten()
        
        candidate_patterns = find_similar_patterns(
            query_emb, embedding_data, device,
            top_k=CONFIG['candidate_count'], target_length=target_length, power=query_length
        )
        
        # v8: Re-ranking Removed
        # No reranking needed for similar patterns cache helper
        
        return {
            'status': 'success',
            'patterns': [pattern_to_frontend_format(
                pattern_info={
                    'start_idx': p['idx'],
                    'similarity': p['sim'],
                    'target_length': target_length
                },
                pattern_data=convert_numpy_types(full_ohlc_data[p['idx']:p['idx']+p['len']]),
                time_info=full_time_data.iloc[p['idx']+p['len']-1],
                forecast_data=convert_numpy_types(full_ohlc_data[p['idx']+p['len']:p['idx']+p['len']+math.ceil(p['len']/3)]) if p['idx']+p['len']+math.ceil(p['len']/3) <= len(full_ohlc_data) else None
            ) for p in candidate_patterns[:top_k]]
        }
        
        return {
            'status': 'success',
            'patterns': [pattern_to_frontend_format(
                pattern_info={
                    'start_idx': p[0],
                    'similarity': p[1],
                    'target_length': target_length
                },
                pattern_data=p[2],
                time_info=p[3],
                forecast_data=p[4] if len(p) > 4 else None
            ) for p in final_patterns]
        }
    except Exception as e:
        raise e

@app.route('/api/similar-patterns', methods=['POST'])
def find_similar_patterns_api():
    """ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰"""
    if model is None or embedding_data is None:
        return jsonify({'error': 'System not initialized'}), 500
    
    try:
        data = request.get_json()
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        query_time = data.get('query_time')
        query_length = data.get('query_length', 5)
        top_k = data.get('top_k', 3)
        target_length = data.get('target_length')
        
        if not query_time:
            # í˜„ì¬ íŒ¨í„´ ì‚¬ìš©
            current_pattern = get_current_pattern()
            if current_pattern is None:
                return jsonify({'error': 'Failed to get current pattern'}), 500
            
            query_normalized = np.array(current_pattern['normalized'])
        else:
            # íŠ¹ì • ì‹œê°„ì˜ íŒ¨í„´ ì‚¬ìš©
            query_start_idx = full_time_data[full_time_data >= pd.to_datetime(query_time)].index[0]
            query_raw = full_ohlc_data[query_start_idx:query_start_idx + query_length]
            query_normalized = normalize_window(query_raw)
        
        # ìºì‹œëœ ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰ ì‚¬ìš©
        query_data_str = ','.join(map(str, query_normalized.flatten()))
        result = _find_similar_patterns_cached(query_data_str, query_length, top_k, target_length)
        
        if result['status'] == 'success':
            return jsonify(result)
        else:
            return jsonify({'error': 'Pattern search failed'}), 500
        
        # numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        similarities = convert_numpy_types(similarities)
        
        print(f"Converted similarities in similar patterns: {similarities}")
        
        # í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result_patterns = []
        for sim_info in similarities:
            pattern_start_idx = sim_info['idx']
            pattern_length = sim_info['len']
            forecast_length = math.ceil(pattern_length / 3)  # íŒ¨í„´ ê¸¸ì´ì˜ 1/3ë§Œí¼ ë¯¸ë˜ ì˜ˆì¸¡
            
            # íŒ¨í„´ ë°ì´í„° (numpy ë°°ì—´ì„ Python listë¡œ ë³€í™˜)
            pattern_data = full_ohlc_data[pattern_start_idx:pattern_start_idx + pattern_length]
            pattern_data = convert_numpy_types(pattern_data)
            # íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œì  (CSVì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
            pattern_completion_time = full_time_data.iloc[pattern_start_idx + pattern_length - 1]
            
            # ë¯¸ë˜ ë°ì´í„° (ì˜ˆì¸¡ êµ¬ê°„)
            forecast_start_idx = pattern_start_idx + pattern_length
            forecast_end_idx = forecast_start_idx + forecast_length
            forecast_data = None
            
            if forecast_end_idx <= len(full_ohlc_data):
                forecast_data = full_ohlc_data[forecast_start_idx:forecast_end_idx]
                forecast_data = convert_numpy_types(forecast_data)
            
            formatted_pattern = pattern_to_frontend_format(
                sim_info, pattern_data, pattern_completion_time, forecast_data
            )
            if formatted_pattern:
                result_patterns.append(formatted_pattern)
        
        result = {
            'query_pattern': current_pattern if not query_time else {
                'timestamp': full_time_data.iloc[query_start_idx + query_length - 1].strftime("%Y.%m.%d %H:%M"),  # ë§ˆì§€ë§‰ ìº”ë“¤ì˜ ì™„ë£Œ ì‹œì 
                'length': query_length,
                'normalized': query_normalized.tolist()
            },
            'similar_patterns': result_patterns
        }
        
        # ìµœì¢… ì•ˆì „ì¥ì¹˜: ëª¨ë“  numpy íƒ€ì… ë³€í™˜
        result = convert_numpy_types(result)
        return jsonify(result)
        
    except Exception as e:
        print(f"Error in similar patterns search: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/historical-analysis', methods=['POST'])
@require_auth(min_tier=UserTier.MEMBER.value)
def historical_analysis_api():
    """íŠ¹ì • ê³¼ê±° ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ íŒ¨í„´ ë¶„ì„ (íšŒì› ì´ìƒ)
    ---
    tags:
      - Analysis
    parameters:
      - name: Authorization
        in: header
        type: string
        required: true
      - name: body
        in: body
        required: true
        schema:
            type: object
            required:
              - historical_point
            properties:
                historical_point:
                    type: string
                    description: Target timestamp (YYYY-MM-DD HH:MM)
                query_length:
                    type: integer
                    default: 5
                top_k:
                    type: integer
                    default: 3
    responses:
      200:
        description: Historical analysis results
        schema:
            type: object
            properties:
                query_pattern:
                    type: object
                similar_patterns:
                    type: array
                    items:
                        type: object
                historical_data_info:
                    type: object
    """
    if model is None or embedding_data is None:
        return jsonify({'error': 'System not initialized'}), 500
    
    try:
        data = request.get_json()
        historical_point = data.get('historical_point')
        query_length = data.get('query_length', 5)
        target_length = data.get('target_length')
        top_k = data.get('top_k', 3)
        
        if not historical_point:
            return jsonify({'error': 'historical_point is required'}), 400
        
        print(f"\n=== Historical Analysis Request ===")
        print(f"  Historical point: {historical_point}")
        print(f"  Query length: {query_length}")
        print(f"  Target length: {target_length}")
        print(f"  Top K: {top_k}")
        
        
        # 1) ì…ë ¥ê°’ì„ KSTë¡œ íŒŒì‹±
        try:
            target_time_naive = pd.to_datetime(historical_point)
            target_time_kst = KST.localize(target_time_naive) if target_time_naive.tz is None else target_time_naive.astimezone(KST)
            print(f"[HIST] Parsed target time (KST): {target_time_kst}")

            # 2) âœ… ì…ë ¥ ì‹œì  'ì§ì „'ì— ì™„ë£Œëœ ìº”ë“¤ì˜ 'ì‹œì‘' ì‹œê°ìœ¼ë¡œ ë³´ì •
            last_completed_start_kst = find_last_completed_candle_start_time_before_point(target_time_kst)
            print(f"[HIST] Adjusted to last completed candle START before target (KST): {last_completed_start_kst.strftime('%Y-%m-%d %H:%M:%S')}")

            # 3) ë‚´ë¶€ ì²˜ë¦¬ìš© naive datetime ë³€í™˜
            target_time = last_completed_start_kst.replace(tzinfo=None)

        except Exception as e:
            print(f"[HIST] ERROR: Failed to parse datetime: {e}")
            return jsonify({'error': 'Invalid datetime format. Use YYYY-MM-DD HH:MM'}), 400
        
        # 2. ìºì‹œ í™•ì¸ ë¨¼ì € ìˆ˜í–‰ (ë°”ì´ë‚¸ìŠ¤ API í˜¸ì¶œ ë°©ì§€)
        # Cache logic removed in v8

        
        # 3. CSV ë°ì´í„° ë²”ìœ„ í™•ì¸
        csv_start_time = full_time_data.iloc[0]
        csv_end_time = full_time_data.iloc[-1]
        print(f"  CSV data range: {csv_start_time} ~ {csv_end_time}")
        
        # 4. ìš”ì²­ ì‹œì ì´ CSV ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        use_binance_api = False
        if target_time < csv_start_time or target_time > csv_end_time:
            print(f"  WARNING: Target time {target_time} is outside CSV range!")
            print(f"  CSV range: {csv_start_time} ~ {csv_end_time}")
            print(f"  Will use Binance API to fetch historical data...")
            use_binance_api = True
        
        if use_binance_api:
            # Binance APIë¥¼ ì‚¬ìš©í•´ì„œ í•´ë‹¹ ì‹œì  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            print(f"  Fetching data from Binance API...")
            if binance_exchange is None:
                return jsonify({'error': 'Binance API not initialized'}), 500
            
            try:
                # âœ… KST ì‹œì ì„ UTC íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜ (ì‹œì‘ ì‹œê° ê¸°ì¤€)
                target_utc_timestamp = convert_kst_to_utc_timestamp(last_completed_start_kst)
                print(f"[HIST] Target UTC timestamp (START): {target_utc_timestamp}")
                
                # query_length + 10ê°œ ì •ë„ ì—¬ìœ ë¶„ì„ ë‘ê³  ê°€ì ¸ì˜¤ê¸° (4h ê¸°ì¤€)
                fetch_limit = query_length + 10
                print(f"[HIST] Fetching {fetch_limit} candles ending at START KST {last_completed_start_kst.strftime('%Y-%m-%d %H:%M:%S')}")
                
                # í•´ë‹¹ ì‹œì ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ëŠ” ìº”ë“¤ë“¤ ê°€ì ¸ì˜¤ê¸°
                start_utc_timestamp = target_utc_timestamp - (fetch_limit * 4 * 60 * 60 * 1000)
                
                # ì•ˆì „í•œ OHLCV ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                ohlcvs = safe_fetch_ohlcv(
                    binance_exchange, 
                    'BTC/USDT', 
                    '4h', 
                    limit=fetch_limit,
                    since=start_utc_timestamp
                )
                print(f"  Fetched {len(ohlcvs)} candles from Binance")
                
                if len(ohlcvs) < query_length:
                    return jsonify({'error': f'Not enough Binance data. Got {len(ohlcvs)}, need {query_length}'}), 400
                
                # target_time ì´ì „ì˜ ìº”ë“¤ë§Œ í•„í„°ë§
                filtered_ohlcvs = []
                for ohlcv in ohlcvs:
                    candle_time = pd.to_datetime(ohlcv[0], unit='ms')
                    if candle_time <= target_time:
                        filtered_ohlcvs.append(ohlcv)
                
                print(f"  Filtered to {len(filtered_ohlcvs)} candles before target time")
                
                if len(filtered_ohlcvs) < query_length:
                    return jsonify({'error': f'Not enough Binance data before target time. Got {len(filtered_ohlcvs)}, need {query_length}'}), 400
                
                # ë§ˆì§€ë§‰ query_lengthê°œ ìº”ë“¤ ì‚¬ìš©
                query_ohlcvs = filtered_ohlcvs[-query_length:]
                query_raw = np.array([[ohlcv[1], ohlcv[2], ohlcv[3], ohlcv[4]] for ohlcv in query_ohlcvs], dtype=np.float32)
                query_time = pd.to_datetime(query_ohlcvs[-1][0], unit='ms')
                
                print(f"  Using Binance data - Query time range: {pd.to_datetime(query_ohlcvs[0][0], unit='ms')} ~ {query_time}")
                print(f"  Query data shape: {query_raw.shape}")
                
                data_source = "Binance API"
                available_times = filtered_ohlcvs  # For consistency in return data
                query_start_global_idx = 0  # Not used for Binance data
                query_end_global_idx = query_length - 1  # Not used for Binance data
                
            except Exception as e:
                print(f"  ERROR: Failed to fetch Binance data: {e}")
                return jsonify({'error': f'Failed to fetch Binance data: {str(e)}'}), 500
        
        else:
            # CSV ë°ì´í„° ì‚¬ìš©
            print(f"  Using CSV data...")
            available_times = full_time_data[full_time_data <= target_time]
            print(f"  Available data points before target time: {len(available_times)}")
            print(f"  Latest available time: {available_times.iloc[-1] if len(available_times) > 0 else 'None'}")
            
            if len(available_times) < query_length:
                return jsonify({'error': f'Not enough historical data. Need at least {query_length} candles before {historical_point}'}), 400
            
            # ê°€ì¥ ë§ˆì§€ë§‰ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ query_lengthë§Œí¼ ì—­ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
            end_idx = len(available_times) - 1
            start_idx = end_idx - query_length + 1
            
            print(f"  Index calculation: start_idx={start_idx}, end_idx={end_idx}")
            
            if start_idx < 0:
                print(f"  ERROR: start_idx < 0. Available: {len(available_times)}, Required: {query_length}")
                return jsonify({'error': f'Not enough data. Available: {len(available_times)}, Required: {query_length}'}), 400
            
            # ì¿¼ë¦¬ íŒ¨í„´ ë°ì´í„° ì¶”ì¶œ
            query_start_global_idx = available_times.index[start_idx]
            query_end_global_idx = available_times.index[end_idx]
            
            print(f"  Global indices: start={query_start_global_idx}, end={query_end_global_idx}")
            
            query_raw = full_ohlc_data[query_start_global_idx:query_end_global_idx + 1]
            query_time = full_time_data.iloc[query_end_global_idx]  # ë§ˆì§€ë§‰ ì‹œì ì„ í‘œì‹œ
            
            print(f"  Query data time range: {full_time_data.iloc[query_start_global_idx]} ~ {full_time_data.iloc[query_end_global_idx]}")
            print(f"  Query data shape: {query_raw.shape}")
            
            data_source = "Historical CSV Data"
        
        # ê³µí†µ ë¡œê·¸ ì¶œë ¥
        print(f"  Query raw data (first 3 candles):")
        for i in range(min(3, len(query_raw))):
            print(f"    Candle #{i+1}: O={query_raw[i,0]:.2f} H={query_raw[i,1]:.2f} L={query_raw[i,2]:.2f} C={query_raw[i,3]:.2f}")
        
        print(f"  Actual query timestamp: {query_time}")
        print(f"  Data source: {data_source}")
        
        # 7. ì •ê·œí™”
        query_normalized = normalize_window(query_raw)
        print(f"  Query normalized data (first 3 candles):")
        for i in range(min(3, len(query_normalized))):
            print(f"    Norm #{i+1}: O={query_normalized[i,0]:.4f} H={query_normalized[i,1]:.4f} L={query_normalized[i,2]:.4f} C={query_normalized[i,3]:.4f}")
        
        # 8. 2ë‹¨ê³„ íŒ¨í„´ ê²€ìƒ‰ (ìºì‹œ í¬í•¨)
        print(f"  Starting 2-stage pattern search with top_k={top_k}, target_length={target_length}")
        similarities = perform_historical_pattern_search(
            historical_point, query_normalized, query_length, top_k, target_length
        )
        
        # numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        similarities = convert_numpy_types(similarities)
        
        print(f"  Found {len(similarities)} similar patterns:")
        for i, sim in enumerate(similarities):
            print(f"    Pattern #{i+1}: idx={sim['idx']}, similarity={sim['sim']:.4f}, length={sim['len']}")
            pattern_start_timestamp = full_time_data.iloc[sim['idx']]
            print(f"      Timestamp: {pattern_start_timestamp}")
        
        # 7. í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result_patterns = []
        print(f"  Converting {len(similarities)} patterns to frontend format...")
        for i, sim_info in enumerate(similarities):
            print(f"    Processing pattern #{i+1}: idx={sim_info['idx']}, similarity={sim_info['sim']:.4f}")
            pattern_start_idx = sim_info['idx']
            pattern_length = sim_info['len']
            forecast_length = math.ceil(pattern_length / 3)
            
            # ê²€ìƒ‰ëœ íŒ¨í„´ì´ í˜„ì¬ ì¿¼ë¦¬ ì‹œì ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸ (CSV ë°ì´í„°ë§Œ)
            if not use_binance_api:
                pattern_end_idx = pattern_start_idx + pattern_length
                if pattern_end_idx > query_start_global_idx:
                    print(f"      Skipping overlapping pattern: idx={pattern_start_idx}, end_idx={pattern_end_idx} > query_start={query_start_global_idx}")
                    continue  # ê²¹ì¹˜ëŠ” íŒ¨í„´ì€ ì œì™¸
            
            # íŒ¨í„´ ë°ì´í„° (numpy ë°°ì—´ì„ Python listë¡œ ë³€í™˜)
            pattern_data = full_ohlc_data[pattern_start_idx:pattern_start_idx + pattern_length]
            pattern_data = convert_numpy_types(pattern_data)
            # íŒ¨í„´ì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì™„ë£Œ ì‹œì  (CSVì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
            pattern_completion_time = full_time_data.iloc[pattern_start_idx + pattern_length - 1]
            
            # ë¯¸ë˜ ë°ì´í„° (ì˜ˆì¸¡ êµ¬ê°„)
            forecast_start_idx = pattern_start_idx + pattern_length
            forecast_end_idx = forecast_start_idx + forecast_length
            forecast_data = None
            
            if forecast_end_idx <= len(full_ohlc_data):
                forecast_data = full_ohlc_data[forecast_start_idx:forecast_end_idx]
                forecast_data = convert_numpy_types(forecast_data)
            
            formatted_pattern = pattern_to_frontend_format(
                sim_info, pattern_data, pattern_completion_time, forecast_data
            )
            if formatted_pattern:
                result_patterns.append(formatted_pattern)
                print(f"      âœ“ Pattern #{i+1} successfully added to results")
            else:
                print(f"      âœ— Pattern #{i+1} failed to format, skipping")
        
        print(f"  Final result: {len(result_patterns)} patterns ready for frontend")
        
        # 8. í˜„ì¬ ì¿¼ë¦¬ íŒ¨í„´ ì •ë³´ ìƒì„±
        candles = []
        for ohlc in query_raw:
            candles.append({
                'open': float(ohlc[0]),
                'high': float(ohlc[1]),
                'low': float(ohlc[2]),
                'close': float(ohlc[3]),
                'volume': 1000000  # ì„ì‹œ ë³¼ë¥¨
            })
        
        # ì¿¼ë¦¬ íŒ¨í„´ ì‹œê°„ë„ í•œêµ­ì‹œê°„ìœ¼ë¡œ ë³€í™˜
        kst = pytz.timezone('Asia/Seoul')
        if hasattr(query_time, 'tz_localize'):
            if query_time.tz is None:
                query_time_kst = query_time.tz_localize('UTC').tz_convert(kst)
            else:
                query_time_kst = query_time.tz_convert(kst)
        else:
            if query_time.tzinfo is None:
                query_time_kst = pytz.UTC.localize(query_time).astimezone(kst)
            else:
                query_time_kst = query_time.astimezone(kst)
                
        if hasattr(target_time, 'tz_localize'):
            if target_time.tz is None:
                target_time_kst = target_time.tz_localize('UTC').tz_convert(kst)
            else:
                target_time_kst = target_time.tz_convert(kst)
        else:
            if target_time.tzinfo is None:
                target_time_kst = pytz.UTC.localize(target_time).astimezone(kst)
            else:
                target_time_kst = target_time.astimezone(kst)

        query_pattern = {
            'timestamp': query_time_kst.strftime("%Y.%m.%d %H:%M"),
            'symbol': f'BTC/USDT (Historical - ìš”ì²­: {target_time_kst.strftime("%Y.%m.%d %H:%M")})',
            'confidence': 95,  # ê³¼ê±° ë°ì´í„°ëŠ” í™•ì‹¤í•¨
            'candles': candles,
            'source': 'historical_data'
        }
        
        result = {
            'query_pattern': query_pattern,
            'similar_patterns': result_patterns,
            'historical_data_info': {
                'requested_point': historical_point,
                'data_source': data_source,
                'actual_query_start': query_time.strftime('%Y-%m-%d %H:%M:%S') if hasattr(query_time, 'strftime') else str(query_time),
                'actual_query_end': query_time.strftime('%Y-%m-%d %H:%M:%S') if hasattr(query_time, 'strftime') else str(query_time),
                'query_length': query_length,
                'available_data_points': len(available_times) if not use_binance_api else len(query_raw),
                'found_patterns': len(result_patterns)
            }
        }
        
        # ìµœì¢… ì•ˆì „ì¥ì¹˜: ëª¨ë“  numpy íƒ€ì… ë³€í™˜
        result = convert_numpy_types(result)
        
        
        return jsonify(result)
        
    except Exception as e:
        print(f"Error in historical analysis: {e}")
        return jsonify({'error': str(e)}), 500

# ì¸ì¦ ê´€ë ¨ APIë“¤
@app.route('/api/auth/send-verification', methods=['POST'])
def send_verification_code():
    """ì´ë©”ì¼ ì¸ì¦ ì½”ë“œ ë°œì†¡"""
    try:
        data = request.get_json()
        email = data.get('email')
        
        if not email:
            return jsonify({'error': 'Email required'}), 400
            
        # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
        import re
        if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
            return jsonify({'error': 'Invalid email format'}), 400
        
        # ê¸°ì¡´ ì‚¬ìš©ì í™•ì¸
        existing_user = db_manager.get_user_by_email(email)
        if existing_user:
            return jsonify({'error': 'Email already exists'}), 409
        
        # ì¬ë°œì†¡ ì œí•œ í™•ì¸ (30ì´ˆ ì¿¨ë‹¤ìš´) ë° ë§Œë£Œëœ ì½”ë“œ ì •ë¦¬
        if email in verification_codes:
            stored_data = verification_codes[email]
            
            # ë§Œë£Œëœ ì½”ë“œ ì •ë¦¬
            if datetime.now() > stored_data.get('expires', datetime.min):
                del verification_codes[email]
            else:
                # 30ì´ˆ ì¿¨ë‹¤ìš´ í™•ì¸
                last_sent = stored_data.get('last_sent', datetime.min)
                if datetime.now() - last_sent < timedelta(seconds=30):
                    return jsonify({'error': 'Please wait 30 seconds before requesting another code'}), 429
        
        # ì¸ì¦ ì½”ë“œ ìƒì„± ë° ì €ì¥
        code = generate_verification_code()
        verification_codes[email] = {
            'code': code,
            'expires': datetime.now() + timedelta(minutes=10),
            'last_sent': datetime.now()
        }
        
        # ì´ë©”ì¼ ë°œì†¡
        if send_verification_email(email, code):
            return jsonify({
                'status': 'success',
                'message': 'Verification code sent successfully'
            })
        else:
            return jsonify({'error': 'Failed to send verification code'}), 500
            
    except Exception as e:
        print(f"Error sending verification code: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/verify-code', methods=['POST'])
def verify_code():
    """ì¸ì¦ ì½”ë“œ ê²€ì¦"""
    try:
        data = request.get_json()
        email = data.get('email')
        code = data.get('code')
        
        if not email or not code:
            return jsonify({'error': 'Email and code required'}), 400
        
        # ì €ì¥ëœ ì¸ì¦ ì½”ë“œ í™•ì¸
        if email not in verification_codes:
            return jsonify({'error': 'No verification code found'}), 404
        
        stored_data = verification_codes[email]
        
        # ë§Œë£Œ ì‹œê°„ í™•ì¸
        if datetime.now() > stored_data['expires']:
            del verification_codes[email]
            return jsonify({'error': 'Verification code expired'}), 410
        
        # ì½”ë“œ ì¼ì¹˜ í™•ì¸
        if stored_data['code'] != code:
            return jsonify({'error': 'Invalid verification code'}), 400
        
        return jsonify({
            'status': 'success',
            'message': 'Code verified successfully'
        })
        
    except Exception as e:
        print(f"Error verifying code: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/register', methods=['POST'])
def register_api():
    """íšŒì› ê°€ì… (ì¸ì¦ ì½”ë“œ ê²€ì¦ í›„)"""
    try:
        data = request.get_json()
        email = data.get('email')
        password = data.get('password')
        verification_code = data.get('verification_code')
        
        if not email or not password or not verification_code:
            return jsonify({'error': 'Email, password and verification code required'}), 400
        
        # ì¸ì¦ ì½”ë“œ ì¬ê²€ì¦
        if email not in verification_codes:
            return jsonify({'error': 'No verification code found. Please request a new code.'}), 404
        
        stored_data = verification_codes[email]
        
        # ë§Œë£Œ ì‹œê°„ í™•ì¸
        if datetime.now() > stored_data['expires']:
            del verification_codes[email]
            return jsonify({'error': 'Verification code expired. Please request a new code.'}), 410
        
        # ì½”ë“œ ì¼ì¹˜ í™•ì¸
        if stored_data['code'] != verification_code:
            return jsonify({'error': 'Invalid verification code'}), 400
        
        # ì¸ì¦ ì„±ê³µ í›„ ì‚¬ìš©ì ë“±ë¡
        user_uuid = db_manager.register_user(email, password)
        
        # ì¸ì¦ ì½”ë“œ ì •ë¦¬
        del verification_codes[email]
        if user_uuid:
            # JWT í† í° ìƒì„±
            payload = {
                'user_uuid': user_uuid,
                'tier': UserTier.MEMBER.value,
                'exp': datetime.now(timezone.utc) + timedelta(days=1)
            }
            token = jwt.encode(payload, app.config['JWT_SECRET_KEY'], algorithm='HS256')
            
            return jsonify({
                'status': 'success',
                'message': 'User registered successfully',
                'token': token
            })
        else:
            return jsonify({'error': 'Email already exists'}), 409
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/login', methods=['POST'])
def login_api():
    """
    User Login
    ---
    tags:
      - Authentication
    parameters:
      - in: body
        name: body
        required: true
        schema:
          type: object
          required:
            - email
            - password
          properties:
            email:
              type: string
              example: user@example.com
            password:
              type: string
              example: password123
    responses:
      200:
        description: Login successful
        schema:
          type: object
          properties:
            status:
              type: string
              example: success
            token:
              type: string
              description: JWT Token
            user_uuid:
              type: string
            tier:
              type: string
            email:
              type: string
            limits:
              type: object
      401:
        description: Invalid credentials
      500:
        description: Internal server error
    """
    try:
        data = request.get_json()
        email = data.get('email')
        password = data.get('password')
        
        if not email or not password:
            return jsonify({'error': 'Email and password required'}), 400
        
        user_uuid, tier = db_manager.login_user(email, password)
        if user_uuid:
            # JWT í† í° ìƒì„±
            payload = {
                'user_uuid': user_uuid,
                'tier': tier,
                'exp': datetime.now(timezone.utc) + timedelta(days=1)
            }
            token = jwt.encode(payload, app.config['JWT_SECRET_KEY'], algorithm='HS256')
            
            # ì‚¬ìš©ì ì •ë³´ë„ í•¨ê»˜ ì¡°íšŒ
            user_info = db_manager.get_user_info(user_uuid)
            
            return jsonify({
                'status': 'success',
                'message': 'Login successful',
                'token': token,
                'user_uuid': user_uuid,
                'email': email,
                'tier': tier,
                'limits': get_tier_limits(tier)
            })
        else:
            return jsonify({'error': 'Invalid credentials'}), 401
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/upgrade', methods=['POST'])
@require_auth(min_tier=UserTier.MEMBER.value)
def upgrade_premium_api():
    """í”„ë¦¬ë¯¸ì—„ ì—…ê·¸ë ˆì´ë“œ"""
    try:
        user_uuid = request.user_info['uuid']
        
        # ì‹¤ì œë¡œëŠ” ê²°ì œ ì‹œìŠ¤í…œê³¼ ì—°ë™
        success = db_manager.upgrade_to_premium(user_uuid, days=30)
        
        if success:
            return jsonify({
                'status': 'success',
                'message': 'Upgraded to premium successfully',
                'new_limits': get_tier_limits(UserTier.PREMIUM.value)
            })
        else:
            return jsonify({'error': 'Upgrade failed'}), 500
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/user/info', methods=['GET'])
@require_auth(min_tier=UserTier.GUEST.value)
def user_info_api():
    """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
    try:
        user_info = request.user_info
        user_uuid = user_info['uuid']
        
        # ì¼ì¼ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
        daily_usage = {
            'live_analysis': db_manager.get_daily_usage_count(user_uuid, '/api/live-analysis'),
            'historical_analysis': db_manager.get_daily_usage_count(user_uuid, '/api/historical-analysis'),
            'total': db_manager.get_daily_usage_count(user_uuid)
        }
        
        return jsonify({
            'user_info': user_info,
            'daily_usage': daily_usage,
            'limits': get_tier_limits(user_info['tier'])
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/train-model', methods=['POST'])
@require_auth(min_tier=UserTier.PREMIUM.value)
def train_model_api():
    """ëª¨ë¸ ì¬í•™ìŠµ API (í”„ë¦¬ë¯¸ì—„ ì „ìš©)"""
    try:
        global model, embedding_data
        
        print("Starting model training...")
        test_ohlc_data = full_ohlc_data[int(len(full_ohlc_data) * 0.9):]
        
        model = train_or_load_model(
            full_ohlc_data, test_ohlc_data, 
            CONFIG['emb_dim'], CONFIG['model_path'], 
            force_train=True, max_len=CONFIG['max_pattern_len']
        )
        
        print("Recomputing embeddings...")
        embedding_data = precompute_and_save_embeddings(
            full_ohlc_data, model, CONFIG['emb_path']
        )
        
        return jsonify({'status': 'success', 'message': 'Model retrained successfully'})
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ======================
# Admin API Endpoints
# ======================

@app.route('/api/admin/users', methods=['GET'])
@require_auth(min_tier='admin')
def admin_list_users():
    """ê´€ë¦¬ì: ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ"""
    try:
        users = db_manager.get_all_users()
        return jsonify({
            'status': 'success',
            'users': users,
            'total_count': len(users)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/user/<user_uuid>/info', methods=['GET'])
@require_auth(min_tier='admin')
def admin_get_user_info(user_uuid):
    """ê´€ë¦¬ì: íŠ¹ì • ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
    try:
        user_info = db_manager.get_user_info(user_uuid)
        if not user_info:
            return jsonify({'error': 'User not found'}), 404
        
        # ì‚¬ìš©ëŸ‰ í†µê³„ ì¶”ê°€
        usage_stats = db_manager.get_user_usage_stats(user_uuid)
        user_info['usage_stats'] = usage_stats
        
        return jsonify({
            'status': 'success',
            'user': user_info
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/user/<user_uuid>/password', methods=['PUT'])
@require_auth(min_tier='admin')
def admin_change_password(user_uuid):
    """ê´€ë¦¬ì: ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ë³€ê²½"""
    try:
        data = request.get_json()
        new_password = data.get('new_password')
        
        if not new_password:
            return jsonify({'error': 'New password is required'}), 400
        
        if len(new_password) < 6:
            return jsonify({'error': 'Password must be at least 6 characters'}), 400
        
        success = db_manager.change_user_password(user_uuid, new_password)
        if not success:
            return jsonify({'error': 'Failed to change password or user not found'}), 404
        
        return jsonify({
            'status': 'success',
            'message': 'Password changed successfully'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/user/<user_uuid>/tier', methods=['PUT'])
@require_auth(min_tier='admin')
def admin_change_tier(user_uuid):
    """ê´€ë¦¬ì: ì‚¬ìš©ì ë“±ê¸‰ ë³€ê²½"""
    try:
        data = request.get_json()
        new_tier = data.get('tier')
        premium_days = data.get('premium_days', 30)  # í”„ë¦¬ë¯¸ì—„ì¸ ê²½ìš° ê¸°ë³¸ 30ì¼
        
        if new_tier not in [UserTier.GUEST.value, UserTier.MEMBER.value, UserTier.PREMIUM.value]:
            return jsonify({'error': 'Invalid tier. Must be guest, member, or premium'}), 400
        
        # í”„ë¦¬ë¯¸ì—„ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ëŠ” ê²½ìš° ë§Œë£Œì¼ ì„¤ì •
        premium_until = None
        if new_tier == UserTier.PREMIUM.value:
            premium_until = datetime.now() + timedelta(days=premium_days)
        
        success = db_manager.change_user_tier(user_uuid, new_tier, premium_until)
        if not success:
            return jsonify({'error': 'Failed to change tier or user not found'}), 404
        
        return jsonify({
            'status': 'success',
            'message': f'User tier changed to {new_tier}',
            'tier': new_tier,
            'premium_until': premium_until.isoformat() if premium_until else None
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/user/<user_uuid>/active', methods=['PUT'])
@require_auth(min_tier='admin')
def admin_toggle_user_active(user_uuid):
    """ê´€ë¦¬ì: ì‚¬ìš©ì í™œì„±í™”/ë¹„í™œì„±í™”"""
    try:
        data = request.get_json()
        is_active = data.get('is_active', True)
        
        success = db_manager.set_user_active_status(user_uuid, is_active)
        if not success:
            return jsonify({'error': 'Failed to update user status or user not found'}), 404
        
        return jsonify({
            'status': 'success',
            'message': f'User {"activated" if is_active else "deactivated"} successfully',
            'is_active': is_active
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/stats', methods=['GET'])
@require_auth(min_tier='admin')
def admin_get_stats():
    """ê´€ë¦¬ì: ì „ì²´ ì‹œìŠ¤í…œ í†µê³„"""
    try:
        stats = db_manager.get_system_stats()
        return jsonify({
            'status': 'success',
            'stats': stats
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/user/<user_uuid>', methods=['DELETE'])
@require_auth(min_tier='admin')
def admin_delete_user(user_uuid):
    """ê´€ë¦¬ì: ì‚¬ìš©ì ì‚­ì œ"""
    try:
        # ìì‹ ì„ ì‚­ì œí•˜ëŠ” ê²ƒì„ ë°©ì§€
        current_user_uuid = request.user_info['uuid']
        if user_uuid == current_user_uuid:
            return jsonify({'error': 'Cannot delete yourself'}), 400
        
        # ì‚¬ìš©ì ì¡´ì¬ í™•ì¸
        user_info = db_manager.get_user_info(user_uuid)
        if not user_info:
            return jsonify({'error': 'User not found'}), 404
        
        # ë‹¤ë¥¸ ê´€ë¦¬ì ì‚­ì œ ë°©ì§€ (ì„ íƒì‚¬í•­)
        if user_info['tier'] == 'admin':
            return jsonify({'error': 'Cannot delete admin users'}), 400
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ì ë° ê´€ë ¨ ë°ì´í„° ì‚­ì œ
        success = db_manager.delete_user(user_uuid)
        
        if success:
            return jsonify({
                'status': 'success',
                'message': f'User {user_info.get("email", user_uuid)} deleted successfully'
            })
        else:
            return jsonify({'error': 'Failed to delete user'}), 500
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/force-auto-analysis', methods=['POST'])
@require_auth(min_tier='admin')
def admin_force_auto_analysis():
    """ê´€ë¦¬ì: ìë™ ë¶„ì„ ê°•ì œ ì‹¤í–‰"""
    try:
        if auto_scheduler:
            auto_scheduler.force_run_now()
            return jsonify({
                'status': 'success',
                'message': 'Auto analysis manually triggered'
            })
        else:
            return jsonify({'error': 'Auto scheduler not initialized'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/system/cleanup', methods=['POST'])
@require_auth(min_tier='admin')
def admin_system_cleanup():
    """ê´€ë¦¬ì: ì‹œìŠ¤í…œ ì •ë¦¬ (ì˜¤ë˜ëœ ê²ŒìŠ¤íŠ¸ ì„¸ì…˜, ë¡œê·¸ ë“±)"""
    try:
        # 30ì¼ ì´ì „ ê²ŒìŠ¤íŠ¸ ì„¸ì…˜ ì •ë¦¬
        cleanup_count = db_manager.cleanup_old_guest_sessions(days=30)
        
        return jsonify({
            'status': 'success',
            'message': f'Cleaned up {cleanup_count} old guest sessions',
            'cleanup_count': cleanup_count
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/users/bulk-action', methods=['POST'])
@require_auth(min_tier='admin')
def admin_bulk_user_action():
    """ê´€ë¦¬ì: ì‚¬ìš©ì ì¼ê´„ ì‘ì—…"""
    try:
        data = request.get_json()
        action = data.get('action')
        user_uuids = data.get('user_uuids', [])
        
        if not action or not user_uuids:
            return jsonify({'error': 'Action and user_uuids are required'}), 400
        
        current_user_uuid = request.user_info['uuid']
        results = []
        
        for user_uuid in user_uuids:
            # ìì‹ ì—ê²ŒëŠ” ì‘ì—…í•˜ì§€ ì•ŠìŒ
            if user_uuid == current_user_uuid:
                results.append({'uuid': user_uuid, 'status': 'skipped', 'reason': 'Cannot modify self'})
                continue
            
            try:
                if action == 'activate':
                    db_manager.set_user_active(user_uuid, True)
                    results.append({'uuid': user_uuid, 'status': 'success', 'action': 'activated'})
                elif action == 'deactivate':
                    db_manager.set_user_active(user_uuid, False)
                    results.append({'uuid': user_uuid, 'status': 'success', 'action': 'deactivated'})
                elif action == 'delete':
                    # ê´€ë¦¬ìëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ
                    user_info = db_manager.get_user_info(user_uuid)
                    if user_info and user_info['tier'] == 'admin':
                        results.append({'uuid': user_uuid, 'status': 'skipped', 'reason': 'Cannot delete admin'})
                    else:
                        db_manager.delete_user(user_uuid)
                        results.append({'uuid': user_uuid, 'status': 'success', 'action': 'deleted'})
                else:
                    results.append({'uuid': user_uuid, 'status': 'error', 'reason': 'Unknown action'})
            except Exception as e:
                results.append({'uuid': user_uuid, 'status': 'error', 'reason': str(e)})
        
        return jsonify({
            'status': 'success',
            'results': results
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


def initialize_system():
    """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
    try:
        print("ğŸ”§ Initializing system components...")
        
        # 1. ë°”ì´ë‚¸ìŠ¤ ì´ˆê¸°í™”
        if not initialize_binance():
            print("âŒ Failed to initialize Binance")
            return False
            
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì´ë¯¸ __init__ì—ì„œ í˜¸ì¶œë¨)
        print("âœ… Database initialized")
        
        # 3. AI ëª¨ë¸ ë° ì„ë² ë”© ë°ì´í„° ì´ˆê¸°í™”
        print("ğŸ¤– Initializing AI model and embeddings...")
        if not initialize_ai_system():
            print("âŒ Failed to initialize AI system")
            return False
        print("âœ… AI system initialized successfully")
        
        # 4. ìë™ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (ì¼ë‹¨ Noneìœ¼ë¡œ íŒ¨ìŠ¤, ë‚˜ì¤‘ì— ìˆ˜ì • ê°€ëŠ¥)
        global auto_scheduler
        auto_scheduler = None  # AutoAnalysisScheduler(pattern_analysis_func) 
        print("âœ… Auto analysis scheduler skipped (can be enabled later)")
        
        print("âœ… All system components initialized successfully")
        return True
        
    except Exception as e:
        print(f"âŒ System initialization failed: {e}")
        return False


# Gunicorn í˜¸í™˜ì„±ì„ ìœ„í•´ ì•± ì‹œì‘ ì‹œ ìë™ ì´ˆê¸°í™”
print("Starting Pattern Detection API Server...")
if not initialize_system():
    print("Failed to initialize system. Please check the configuration.")

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    print(f"Server starting on http://localhost:{port}")
    debug_mode = os.environ.get('FLASK_ENV') == 'development'
    app.run(host='0.0.0.0', port=port, debug=debug_mode, use_reloader=False)